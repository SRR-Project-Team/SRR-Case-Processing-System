#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIä¸»é¢˜classifyå™¨ - J_subject_matterfieldæ™ºèƒ½classify
åŸºäºå†å²dataå’Œè§„åˆ™è¿›è¡Œæ™ºèƒ½classifyï¼Œæ”¯æŒ17ä¸ªé¢„å®šä¹‰classåˆ«
"""

import pandas as pd

def load_srr_rules():
    """åŠ è½½SRRè§„åˆ™"""
    import json
    import os
    
    rules_file = 'models/config/srr_rules.json'
    if os.path.exists(rules_file):
        with open(rules_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print("âš ï¸ SRRè§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨")
        return {'content': [], 'paragraphs': 0}


def load_training_data():
    """åŠ è½½trainingdata"""
    import pickle
    import os
    
    data_file = 'models/ai_models/training_data.pkl'
    if os.path.exists(data_file):
        with open(data_file, 'rb') as f:
            data = pickle.load(f)
        return data.get('srr_data', []), data.get('complaints_data', [])
    else:
        print("âš ï¸ trainingdataæ–‡ä»¶ä¸å­˜åœ¨")
        return [], []

import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
import numpy as np
from typing import Dict, Any, List, Tuple, Optional
import os
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils.file_utils import read_file_with_encoding
from .ai_model_cache import get_cached_model, cache_model


# é¢„å®šä¹‰çš„ä¸»é¢˜classåˆ«map
SUBJECT_MATTER_CATEGORIES = {
    0: "Endangered Tree",
    1: "Drainage Blockage", 
    2: "Fallen Tree",
    3: "Grass Cutting",
    4: "Remove Debris",
    5: "Mosquito Breeding",
    6: "Tree Trimming/ Pruning",
    7: "Landslide",
    8: "Fallen Rock/ Boulders",
    9: "Water Seepage",
    10: "Hazardous tree",
    11: "Others",
    12: "Tree Transplantation / Felling",
    13: "Cracked slope/Wall Surface",
    14: "Repair slope fixture/furniture",
    15: "Surface erosion",
    16: "Repeated case",
    17: "Reminder for outstanding works"
}

# åå‘map
CATEGORY_TO_ID = {v: k for k, v in SUBJECT_MATTER_CATEGORIES.items()}


def load_historical_subject_data(data_path: str) -> pd.DataFrame:
    """
    åŠ è½½å†å²ä¸»é¢˜classifydata
    
    Args:
        data_path: datafile path
        
    Returns:
        pd.DataFrame: æ¸…æ´—åçš„å†å²data
    """
    try:
        print(f"ğŸ“Š åŠ è½½å†å²ä¸»é¢˜data: {data_path}")
        
        if data_path.endswith('.csv'):
            # readCSVfile
            csv_content = read_file_with_encoding(data_path)
            with open('temp_subject_data.csv', 'w', encoding='utf-8') as f:
                f.write(csv_content)
            df = pd.read_csv('temp_subject_data.csv')
            os.remove('temp_subject_data.csv')
        else:
            # readExcelfile
            df = pd.read_excel(data_path, usecols='A:AZ')
        
        print(f"âœ… åŸå§‹dataåŠ è½½success: {len(df)} æ¡record")
        
        # findç›¸å…³åˆ—
        nature_col = None
        aims_col = None
        
        for col in df.columns:
            if 'nature of complaint' in col.lower():
                nature_col = col
            elif 'aims complaint type' in col.lower():
                aims_col = col
        
        if not nature_col and not aims_col:
            print("âš ï¸ æœªæ‰¾åˆ°ä¸»é¢˜ç›¸å…³åˆ—ï¼Œä½¿ç”¨é»˜è®¤data")
            return pd.DataFrame()
        
        # æ¸…æ´—data
        cleaned_data = []
        
        # ä½¿ç”¨AIMS Complaint Typeä½œä¸ºmaindataæº
        if aims_col:
            aims_data = df[aims_col].dropna()
            for complaint_type in aims_data:
                if complaint_type and str(complaint_type).strip():
                    cleaned_data.append({
                        'complaint_text': str(complaint_type).strip(),
                        'source': 'AIMS'
                    })
        
        # è¡¥å……Nature of complaintdata
        if nature_col:
            nature_data = df[nature_col].dropna()
            for nature in nature_data:
                if nature and str(nature).strip():
                    cleaned_data.append({
                        'complaint_text': str(nature).strip(),
                        'source': 'Nature'
                    })
        
        result_df = pd.DataFrame(cleaned_data)
        print(f"âœ… æ¸…æ´—ådata: {len(result_df)} æ¡record")
        
        return result_df
        
    except Exception as e:
        print(f"âŒ åŠ è½½å†å²datafailed: {e}")
        return pd.DataFrame()


def create_keyword_mapping() -> Dict[str, List[str]]:
    """
    åˆ›å»ºå…³é”®è¯åˆ°classåˆ«çš„æ˜ å°„
    
    Returns:
        Dict[str, List[str]]: classåˆ«åˆ°å…³é”®è¯çš„æ˜ å°„
    """
    keyword_mapping = {
        "Endangered Tree": [
            "endangered tree", "å±é™©æ ‘æœ¨", "tree danger", "tree risk", "unstable tree"
        ],
        "Drainage Blockage": [
            "drainage", "blockage", "blocked drain", "æ’æ°´", "å µå¡", "drainage clearance",
            "drain block", "water block", "æ’æ°´å µå¡"
        ],
        "Fallen Tree": [
            "fallen tree", "tree fall", "å€’å¡Œæ ‘æœ¨", "fallen trees", "tree fallen",
            "tree collapse", "å€’æ ‘"
        ],
        "Grass Cutting": [
            "grass cutting", "grass cut", "trimming", "å‰²è‰", "ä¿®å‰ªè‰åª", 
            "grass maintenance", "lawn cutting"
        ],
        "Remove Debris": [
            "remove debris", "debris removal", "æ¸…ç†ç¢ç‰‡", "remove refuse", 
            "rubbish removal", "æ¸…ç†åƒåœ¾", "debris clear"
        ],
        "Mosquito Breeding": [
            "mosquito", "breeding", "èšŠè™«æ»‹ç”Ÿ", "mosquito control", "pest control",
            "insect breeding"
        ],
        "Tree Trimming/ Pruning": [
            "tree trimming", "pruning", "tree pruning", "ä¿®å‰ªæ ‘æœ¨", "tree maintenance",
            "branch cutting", "tree cut"
        ],
        "Landslide": [
            "landslide", "slope failure", "å±±æ³¥å€¾æ³»", "åœŸçŸ³æµ", "slope collapse",
            "land slip", "slope instability"
        ],
        "Fallen Rock/ Boulders": [
            "fallen rock", "boulder", "rock fall", "çŸ³å¤´æ‰è½", "å²©çŸ³æ»‘è½",
            "rock slide", "stone fall"
        ],
        "Water Seepage": [
            "water seepage", "seepage", "æ¸—æ°´", "water leak", "water infiltration",
            "observe water seepage", "water penetration"
        ],
        "Hazardous tree": [
            "hazardous tree", "dangerous tree", "tree hazard", "å±é™©æ ‘æœ¨",
            "unsafe tree", "tree safety"
        ],
        "Tree Transplantation / Felling": [
            "tree transplantation", "tree felling", "tree removal", "ç§»æ¤æ ‘æœ¨",
            "ç ä¼æ ‘æœ¨", "tree relocation"
        ],
        "Cracked slope/Wall Surface": [
            "crack", "cracked slope", "wall crack", "è£‚ç¼", "slope crack",
            "surface crack", "wall surface"
        ],
        "Repair slope fixture/furniture": [
            "repair", "slope fixture", "furniture", "ç»´ä¿®", "slope maintenance",
            "fixture repair", "slope repair"
        ],
        "Surface erosion": [
            "erosion", "surface erosion", "åœŸå£¤ä¾µèš€", "slope erosion",
            "surface wear", "weathering"
        ],
        "Repeated case": [
            "repeated", "repeat case", "é‡å¤æ¡ˆä¾‹", "duplicate", "recurring",
            "repeated case"
        ],
        "Reminder for outstanding works": [
            "reminder", "outstanding", "follow up", "æé†’", "æœªå®Œæˆå·¥ä½œ",
            "pending work", "outstanding work"
        ],
        "Others": [
            "others", "other", "å…¶ä»–", "miscellaneous", "general", "unspecified"
        ]
    }
    
    return keyword_mapping


class SubjectMatterClassifier:
    """ä¸»é¢˜classifyå™¨"""
    
    def __init__(self, historical_data_paths: List[str]):
        """
        initializeclassifyå™¨ï¼Œä½¿ç”¨cacheä¼˜åŒ–
        
        Args:
            historical_data_paths: å†å²datafile pathåˆ—table
        """
        # å°è¯•ä»cachegetclassifyå™¨
        cache_key = "subject_matter_classifier"
        cached_classifier = get_cached_model(cache_key)
        
        if cached_classifier:
            # ä½¿ç”¨cacheçš„classifyå™¨
            self.historical_data = cached_classifier.get('historical_data')
            self.keyword_mapping = cached_classifier.get('keyword_mapping')
            self.vectorizer = cached_classifier.get('vectorizer')
            self.model = cached_classifier.get('model')
            self.label_encoder = cached_classifier.get('label_encoder')
            print("ğŸš€ ä½¿ç”¨cacheçš„ä¸»é¢˜classifyå™¨")
            return
        
        # cacheæœªå‘½ä¸­ï¼Œæ­£å¸¸initialize
        self.historical_data = self._load_all_historical_data(historical_data_paths)
        self.keyword_mapping = create_keyword_mapping()
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.label_encoder = LabelEncoder()
        self._train_model()
        
        # cacheclassifyå™¨
        try:
            classifier_cache = {
                'historical_data': self.historical_data,
                'keyword_mapping': self.keyword_mapping,
                'vectorizer': self.vectorizer,
                'model': self.model,
                'label_encoder': self.label_encoder
            }
            cache_model(cache_key, classifier_cache)
        except Exception as e:
            print(f"âš ï¸ cacheä¸»é¢˜classifyå™¨failed: {e}")
    
    def _load_all_historical_data(self, data_paths: List[str]) -> pd.DataFrame:
        """åŠ è½½æ‰€æœ‰å†å²data"""
        all_data = []
        
        for path in data_paths:
            if os.path.exists(path):
                df = load_historical_subject_data(path)
                if not df.empty:
                    all_data.append(df)
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            print(f"âœ… æ€»å†å²data: {len(combined_df)} æ¡record")
            return combined_df
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆå†å²data")
            return pd.DataFrame()
    
    def _preprocess_text(self, text: str) -> str:
        """é¢„processæ–‡æœ¬"""
        if not text:
            return ""
        
        # è½¬æ¢ä¸ºå°å†™
        text = str(text).lower()
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _map_to_standard_category(self, complaint_text: str) -> str:
        """å°†å†å²dataæ˜ å°„åˆ°æ ‡å‡†classåˆ«"""
        text_lower = complaint_text.lower()
        
        # ç›´æ¥match
        for category, keywords in self.keyword_mapping.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    return category
        
        # ç‰¹æ®Šmapè§„åˆ™
        mapping_rules = {
            'trimming': 'Tree Trimming/ Pruning',
            'withered tree': 'Hazardous tree',
            'to observe': 'Others',
            'slope maintenance': 'Repair slope fixture/furniture',
            'drainage clearance': 'Drainage Blockage',
            'remove refuse': 'Remove Debris'
        }
        
        for pattern, category in mapping_rules.items():
            if pattern in text_lower:
                return category
        
        return "Others"
    
    def _train_model(self):
        """trainingæœºå™¨å­¦ä¹ model"""
        if self.historical_data.empty:
            print("âš ï¸ æ— å†å²dataï¼Œä»…ä½¿ç”¨è§„åˆ™classify")
            return
        
        print("ğŸ¤– trainingä¸»é¢˜classifymodel...")
        
        # å‡†å¤‡trainingdata
        texts = []
        labels = []
        
        for _, row in self.historical_data.iterrows():
            complaint_text = row['complaint_text']
            processed_text = self._preprocess_text(complaint_text)
            
            if processed_text:
                # mapåˆ°æ ‡å‡†classåˆ«
                category = self._map_to_standard_category(complaint_text)
                texts.append(processed_text)
                labels.append(category)
        
        if len(texts) < 10:
            print("âš ï¸ trainingdataä¸è¶³ï¼Œä»…ä½¿ç”¨è§„åˆ™classify")
            return
        
        # encodingæ ‡ç­¾
        encoded_labels = self.label_encoder.fit_transform(labels)
        
        # vectoråŒ–æ–‡æœ¬
        X = self.vectorizer.fit_transform(texts)
        
        # splittrainingtesté›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels
        )
        
        # trainingmodel
        self.model.fit(X_train, y_train)
        
        # evaluatemodel
        y_pred = self.model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        print(f"âœ… modeltrainingå®Œæˆï¼Œaccuracy: {accuracy:.2f}")
        
        # æ˜¾ç¤ºclassifyæŠ¥å‘Š
        try:
            target_names = self.label_encoder.classes_
            print("\nmodelè¯„ä¼°:")
            print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))
        except Exception as e:
            print(f"\nâš ï¸ classifyæŠ¥å‘Šç”Ÿæˆfailed: {e}")
            print(f"modelaccuracy: {accuracy:.2f}")
    
    def _rule_based_classify(self, case_data: Dict[str, Any]) -> Tuple[Optional[str], float, str]:
        """åŸºäºè§„åˆ™çš„classify"""
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬information
        text_sources = [
            case_data.get('I_nature_of_request', ''),
            case_data.get('J_subject_matter', ''),
            case_data.get('Q_case_details', ''),
            case_data.get('content', '')
        ]
        
        combined_text = ' '.join(filter(None, text_sources)).lower()
        
        if not combined_text.strip():
            return None, 0.0, "no_content"
        
        # å…³keyè¯matchè¯„åˆ†
        category_scores = {}
        
        for category, keywords in self.keyword_mapping.items():
            score = 0
            matched_keywords = []
            
            for keyword in keywords:
                if keyword.lower() in combined_text:
                    # æ ¹æ®å…³keyè¯é‡è¦æ€§ç»™åˆ†
                    if len(keyword) > 10:  # é•¿å…³é”®è¯æ›´ç²¾ç¡®
                        score += 3
                    elif len(keyword) > 5:
                        score += 2
                    else:
                        score += 1
                    matched_keywords.append(keyword)
            
            if score > 0:
                category_scores[category] = {
                    'score': score,
                    'keywords': matched_keywords
                }
        
        if category_scores:
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„classåˆ«
            best_category = max(category_scores.keys(), key=lambda x: category_scores[x]['score'])
            max_score = category_scores[best_category]['score']
            confidence = min(max_score / 10.0, 1.0)  # å½’ä¸€åŒ–confidence
            
            return best_category, confidence, f"rule_based (keywords: {category_scores[best_category]['keywords'][:3]})"
        
        return None, 0.0, "no_match"
    
    def _ml_classify(self, case_data: Dict[str, Any]) -> Tuple[str, float, str]:
        """åŸºäºæœºå™¨å­¦ä¹ çš„classify"""
        if not hasattr(self.model, 'predict'):
            return "Others", 0.3, "ml_not_available"
        
        # æ”¶é›†æ–‡æœ¬information
        text_sources = [
            case_data.get('I_nature_of_request', ''),
            case_data.get('J_subject_matter', ''),
            case_data.get('Q_case_details', ''),
            case_data.get('content', '')
        ]
        
        combined_text = ' '.join(filter(None, text_sources))
        processed_text = self._preprocess_text(combined_text)
        
        if not processed_text:
            return "Others", 0.3, "no_text_for_ml"
        
        try:
            # vectoråŒ–
            X = self.vectorizer.transform([processed_text])
            
            # prediction
            prediction = self.model.predict(X)[0]
            probabilities = self.model.predict_proba(X)[0]
            
            # decodingæ ‡ç­¾
            predicted_category = self.label_encoder.inverse_transform([prediction])[0]
            confidence = max(probabilities)
            
            return predicted_category, confidence, f"machine_learning"
            
        except Exception as e:
            print(f"âš ï¸ MLclassifyfailed: {e}")
            return "Others", 0.3, "ml_error"
    
    def classify(self, case_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¸»classifymethod
        
        Args:
            case_data: æ¡ˆä»¶data
            
        Returns:
            Dict: classifyresult
        """
        print("ğŸ” å¼€å§‹ä¸»é¢˜classify...")
        
        # 1. å°è¯•è§„åˆ™classify
        rule_result, rule_confidence, rule_method = self._rule_based_classify(case_data)
        
        # 2. å°è¯•MLclassify
        ml_result, ml_confidence, ml_method = self._ml_classify(case_data)
        
        # 3. å†³ç­–é€»è¾‘
        if rule_result and rule_confidence >= 0.7:
            # é«˜confidenceè§„åˆ™classify
            final_category = rule_result
            final_confidence = rule_confidence
            final_method = rule_method
        elif rule_result and ml_result == rule_result:
            # è§„åˆ™å’ŒMLä¸€è‡´
            final_category = rule_result
            final_confidence = (rule_confidence + ml_confidence) / 2
            final_method = f"consensus ({rule_method} + {ml_method})"
        elif ml_confidence >= 0.6:
            # é«˜confidenceMLclassify
            final_category = ml_result
            final_confidence = ml_confidence
            final_method = ml_method
        elif rule_result:
            # ä½¿ç”¨è§„åˆ™classify
            final_category = rule_result
            final_confidence = rule_confidence
            final_method = rule_method
        else:
            # é»˜è®¤classify
            final_category = "Others"
            final_confidence = 0.3
            final_method = "default"
        
        # getclassåˆ«ID
        category_id = CATEGORY_TO_ID.get(final_category, 11)  # é»˜è®¤ä¸ºOthers
        
        result = {
            'predicted_category': final_category,
            'category_id': category_id,
            'confidence': final_confidence,
            'method': final_method,
            'rule_result': rule_result,
            'ml_result': ml_result
        }
        
        print(f"âœ… ä¸»é¢˜classifyå®Œæˆ: {final_category} (ID: {category_id}, confidence: {final_confidence:.2f})")
        
        return result


def classify_subject_matter_ai(case_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    AIä¸»é¢˜classifyå…¥å£å‡½æ•°
    
    Args:
        case_data: æ¡ˆä»¶dataï¼ŒåŒ…å«ä»¥ä¸‹fieldï¼š
            - I_nature_of_request: è¯·æ±‚æ€§è´¨
            - J_subject_matter: ä¸»é¢˜äº‹é¡¹
            - Q_case_details: æ¡ˆä»¶è¯¦æƒ…
            - content: åŸå§‹å†…å®¹
            
    Returns:
        Dict: classifyresult
        {
            'predicted_category': str,  # predictionclassåˆ«åç§°
            'category_id': int,         # classåˆ«ID
            'confidence': float,        # confidence
            'method': str,              # classifymethod
            'rule_result': str,         # è§„åˆ™classifyresult
            'ml_result': str            # MLclassifyresult
        }
    """
    try:
        # å†å²dataè·¯å¾„
        historical_data_paths = [
            'models/ai_models/training_data.pkl',
            'models/ai_models/training_data.pkl'
        ]
        
        # createclassifyå™¨
        classifier = SubjectMatterClassifier(historical_data_paths)
        
        # æ‰§è¡Œclassify
        result = classifier.classify(case_data)
        
        return result
        
    except Exception as e:
        print(f"âŒ ä¸»é¢˜classifyfailed: {e}")
        return {
            'predicted_category': 'Others',
            'category_id': 11,
            'confidence': 0.3,
            'method': 'error_fallback',
            'rule_result': None,
            'ml_result': None
        }


def test_subject_matter_classifier():
    """æµ‹è¯•ä¸»é¢˜classifyå™¨"""
    print("=== ä¸»é¢˜classifyå™¨æµ‹è¯• ===\n")
    
    # testç”¨ä¾‹
    test_cases = [
        {
            'name': 'è‰åªä¿®å‰ª',
            'data': {
                'I_nature_of_request': 'Request for grass cutting maintenance',
                'content': 'The grass on the slope is overgrown and needs cutting'
            }
        },
        {
            'name': 'æ ‘æœ¨å€’å¡Œ',
            'data': {
                'I_nature_of_request': 'Report fallen tree on slope',
                'Q_case_details': 'A large tree has fallen and is blocking the path'
            }
        },
        {
            'name': 'æ’æ°´å µå¡',
            'data': {
                'I_nature_of_request': 'Drainage system blocked',
                'content': 'Water cannot flow properly due to blockage in drainage'
            }
        },
        {
            'name': 'å±é™©æ ‘æœ¨',
            'data': {
                'I_nature_of_request': 'Hazardous tree needs attention',
                'content': 'Tree appears unstable and poses safety risk'
            }
        },
        {
            'name': 'æ¸—æ°´é—®é¢˜',
            'data': {
                'I_nature_of_request': 'Water seepage observed',
                'content': 'Water is seeping through the slope wall'
            }
        }
    ]
    
    for test_case in test_cases:
        print(f"ğŸ“‹ æµ‹è¯•æ¡ˆä¾‹: {test_case['name']}")
        result = classify_subject_matter_ai(test_case['data'])
        
        print(f"   predictionclassåˆ«: {result['predicted_category']}")
        print(f"   classåˆ«ID: {result['category_id']}")
        print(f"   confidence: {result['confidence']:.2f}")
        print(f"   classifymethod: {result['method']}")
        print()


if __name__ == "__main__":
    test_subject_matter_classifier()
