"""
AIæ¡ˆä»¶classå‹classifyå™¨module

æœ¬moduleå®ç°åŸºäºmachine learningå’Œè§„åˆ™matchçš„æ™ºèƒ½æ¡ˆä»¶classå‹classifyç³»ç»Ÿï¼Œèƒ½å¤Ÿautomaticå°†SRRæ¡ˆä»¶
classifyä¸ºEmergencyï¼ˆç´§æ€¥ï¼‰ã€Urgentï¼ˆç´§æ€¥ï¼‰æˆ–Generalï¼ˆä¸€èˆ¬ï¼‰ä¸‰ç§classå‹ã€‚

mainfunctionï¼š
1. åŸºäºå†å²æ¡ˆä»¶dataè¿›è¡Œmachine learningtraining
2. ç»“åˆSRRè§„åˆ™æ–‡æ¡£è¿›è¡Œè§„åˆ™match
3. ä½¿ç”¨æ··åˆmethodï¼ˆML + è§„åˆ™ï¼‰æé«˜classifyaccuracy
4. æ”¯æŒmodelcacheå’Œå¢é‡å­¦ä¹ 
5. æä¾›è¯¦ç»†çš„classifyæŠ¥å‘Šå’Œconfidenceevaluate

æŠ€æœ¯å®ç°ï¼š
- machine learningï¼šRandomForestClassifier + TF-IDFvectoråŒ–
- è§„åˆ™matchï¼šåŸºäºå…³keyè¯å’Œè¯­ä¹‰çš„è§„åˆ™å¼•æ“
- dataæ¥æºï¼šSRRå†å²data + æŠ•è¯‰æ¡ˆä»¶data + SRRè§„åˆ™æ–‡æ¡£

ä½œè€…: Project3 Team
ç‰ˆæœ¬: 2.0
"""

import pandas as pd

def load_srr_rules():
    """
    åŠ è½½SRRè§„åˆ™æ–‡æ¡£data
    
    ä»é¢„processçš„JSONæ–‡ä»¶ä¸­åŠ è½½SRRè§„åˆ™å†…å®¹ï¼Œè¿™äº›è§„åˆ™ç”¨äºè§„åˆ™åŒ¹é…classifyã€‚
    è§„åˆ™æ–‡ä»¶åŒ…å«ä»SRR rules.docxæ–‡æ¡£ä¸­extractçš„å…³é”®è¯å’Œclassifyæ ‡å‡†ã€‚
    
    Returns:
        dict: åŒ…å«è§„åˆ™å†…å®¹çš„å­—å…¸
        {
            'content': list,  # è§„åˆ™text contentåˆ—table
            'paragraphs': int  # æ®µè½æ•°é‡
        }
        
    Example:
        >>> rules = load_srr_rules()
        >>> print(f"åŠ è½½äº† {rules['paragraphs']} ä¸ªè§„åˆ™æ®µè½")
    """
    import json
    import os
    
    rules_file = 'models/config/srr_rules.json'
    if os.path.exists(rules_file):
        with open(rules_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print("âš ï¸ SRRè§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨")
        return {'content': [], 'paragraphs': 0}


def load_training_data():
    """
    åŠ è½½æœºå™¨å­¦ä¹ trainingdata
    
    ä»é¢„processçš„pickleæ–‡ä»¶ä¸­åŠ è½½å†å²æ¡ˆä»¶dataï¼Œç”¨äºtrainingclassifymodelã€‚
    dataåŒ…å«SRRæ¡ˆä»¶dataå’ŒæŠ•è¯‰æ¡ˆä»¶dataï¼Œå·²è¿›è¡Œæ¸…æ´—å’Œé¢„processã€‚
    
    Returns:
        tuple: (srr_data, complaints_data)
            - srr_data (list): SRRæ¡ˆä»¶dataåˆ—table
            - complaints_data (list): æŠ•è¯‰æ¡ˆä»¶dataåˆ—table
            
    Example:
        >>> srr_data, complaints_data = load_training_data()
        >>> print(f"SRRdata: {len(srr_data)}æ¡, æŠ•è¯‰data: {len(complaints_data)}æ¡")
    """
    import pickle
    import os
    
    data_file = 'models/ai_models/training_data.pkl'
    if os.path.exists(data_file):
        with open(data_file, 'rb') as f:
            data = pickle.load(f)
        return data.get('srr_data', []), data.get('complaints_data', [])
    else:
        print("âš ï¸ trainingdataæ–‡ä»¶ä¸å­˜åœ¨")
        return [], []

import numpy as np
import re
from typing import Dict, List, Optional, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pickle
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')
from .ai_model_cache import get_cached_model, cache_model

class SRRCaseTypeClassifier:
    """SRRæ¡ˆä»¶classå‹AIclassifyå™¨"""
    
    def __init__(self, data_path: str = "models"):
        self.data_path = data_path
        self.model = None
        self.vectorizer = None
        self.label_encoder = None
        self.feature_names = []
        self.classification_rules = {}
        self.historical_data = None
        
        # ç´§æ€¥å…³keyè¯ (Emergency indicators)
        self.emergency_keywords = [
            'collapse', 'collapsed', 'falling', 'fallen', 'immediate danger', 
            'urgent repair', 'safety risk', 'hazard', 'emergency', 'critical',
            'å€’å¡Œ', 'å´©å¡Œ', 'ç·Šæ€¥', 'å±éšª', 'ç«‹å³', 'å®‰å…¨é¢¨éšª', 'åš´é‡', 'ç·Šæ€¥ä¿®å¾©'
        ]
        
        # ç´§æ€¥æ¡ˆä»¶classå‹
        self.urgent_types = [
            'hazardous tree', 'fallen tree', 'drainage blockage', 'water seepage',
            'remove debris', 'safety concern', 'structural damage'
        ]
        
        # ä¸€èˆ¬æ¡ˆä»¶classå‹  
        self.general_types = [
            'grass cutting', 'tree trimming', 'pruning', 'maintenance',
            'routine inspection', 'general enquiry', 'information request'
        ]
        
    def load_historical_data(self) -> pd.DataFrame:
        """åŠ è½½å†å²data"""
        try:
            # loadSRRå†å²data
            csv_path = os.path.join(self.data_path, "SRR data 2021-2024.csv")
            df = pd.read_csv(csv_path, encoding='latin1')
            
            print(f"âœ… åŠ è½½å†å²datasuccess: {len(df)} æ¡record")
            
            # cleanupåˆ—å
            df.columns = [col.strip().replace('\n', ' ') for col in df.columns]
            
            # æ‰¾åˆ°classå‹åˆ—
            type_col = None
            for col in df.columns:
                if 'type' in col.lower() and ('emergency' in col.lower() or 'urgent' in col.lower()):
                    type_col = col
                    break
            
            if type_col:
                # cleanupclasså‹data
                df[type_col] = df[type_col].fillna('General')
                df = df[df[type_col].isin(['Emergency', 'Urgent', 'General'])]
                
                print(f"classå‹åˆ†å¸ƒ:")
                print(df[type_col].value_counts())
                
            self.historical_data = df
            return df
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å†å²datafailed: {e}")
            return pd.DataFrame()
    
    def load_srr_rules(self) -> Dict:
        """åŠ è½½SRRè§„åˆ™"""
        try:
            from docx import Document
            
            rules_path = os.path.join(self.data_path, "SRR rules.docx")
            doc = Document(rules_path)
            
            rules = {
                'emergency_criteria': [],
                'urgent_criteria': [],
                'general_criteria': []
            }
            
            current_section = None
            for para in doc.paragraphs:
                text = para.text.strip().lower()
                if not text:
                    continue
                    
                # è¯†åˆ«è§„åˆ™éƒ¨åˆ†
                if 'emergency' in text:
                    current_section = 'emergency_criteria'
                elif 'urgent' in text:
                    current_section = 'urgent_criteria'
                elif 'general' in text:
                    current_section = 'general_criteria'
                elif current_section and len(text) > 10:
                    rules[current_section].append(text)
            
            print(f"âœ… åŠ è½½SRRè§„åˆ™success")
            for section, items in rules.items():
                print(f"{section}: {len(items)} æ¡è§„åˆ™")
                
            self.classification_rules = rules
            return rules
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½SRRè§„åˆ™failed: {e}")
            # ä½¿ç”¨é»˜è®¤è§„åˆ™
            return self._get_default_rules()
    
    def _get_default_rules(self) -> Dict:
        """è·å–é»˜è®¤classifyè§„åˆ™"""
        return {
            'emergency_criteria': [
                'immediate safety risk to public',
                'structural collapse or imminent collapse',
                'fallen tree blocking road or causing danger',
                'severe water seepage causing instability',
                'major drainage blockage causing flooding'
            ],
            'urgent_criteria': [
                'hazardous tree requiring immediate attention',
                'water seepage requiring prompt investigation',
                'drainage blockage affecting slope stability',
                'debris removal for safety reasons',
                'slope maintenance affecting public safety'
            ],
            'general_criteria': [
                'routine grass cutting and maintenance',
                'general tree trimming and pruning',
                'regular slope inspection',
                'information enquiry',
                'non-urgent maintenance work'
            ]
        }
    
    def extract_features(self, case_data: Dict) -> Dict:
        """ä»æ¡ˆä»¶dataä¸­extractfeature"""
        features = {}
        
        # æ–‡æœ¬feature
        text_fields = [
            case_data.get('I_nature_of_request', ''),
            case_data.get('J_subject_matter', ''),
            case_data.get('Q_case_details', ''),
            case_data.get('B_source', '')
        ]
        
        combined_text = ' '.join(str(field) for field in text_fields).lower()
        
        # ç´§æ€¥å…³keyè¯è®¡æ•°
        emergency_count = sum(1 for keyword in self.emergency_keywords 
                            if keyword.lower() in combined_text)
        features['emergency_keywords'] = emergency_count
        
        # æ¡ˆä»¶æ¥æºfeature
        source = case_data.get('B_source', '').lower()
        features['source_rcc'] = 1 if 'rcc' in source else 0
        features['source_icc'] = 1 if 'icc' in source else 0
        features['source_1823'] = 1 if '1823' in source else 0
        
        # æ—¶é—´feature (å‘¨æœ«/èŠ‚å‡æ—¥å¯èƒ½æ›´ç´§æ€¥)
        try:
            date_str = case_data.get('A_date_received', '')
            if date_str:
                # ç®€å•çš„æ—¶é—´feature
                features['is_weekend'] = 0  # å¯ä»¥æ ¹æ®å®é™…æ—¥æœŸè®¡ç®—
        except:
            features['is_weekend'] = 0
        
        # æ–œå¡ç¼–å·feature (æŸäº›åŒºåŸŸå¯èƒ½æ›´å®¹æ˜“æœ‰ç´§æ€¥æƒ…å†µ)
        slope_no = case_data.get('G_slope_no', '')
        features['has_slope_no'] = 1 if slope_no else 0
        
        # è”ç³»informationfeature (æœ‰è”ç³»æ–¹å¼å¯èƒ½æ›´ç´§æ€¥)
        contact = case_data.get('F_contact_no', '')
        features['has_contact'] = 1 if contact else 0
        
        # æ–‡æœ¬é•¿åº¦feature (è¯¦ç»†æè¿°å¯èƒ½tableç¤ºæ›´å¤æ‚çš„é—®é¢˜)
        features['text_length'] = len(combined_text)
        features['word_count'] = len(combined_text.split())
        
        return features
    
    def rule_based_classification(self, case_data: Dict) -> Tuple[str, float]:
        """åŸºäºè§„åˆ™çš„classify"""
        text_fields = [
            case_data.get('I_nature_of_request', ''),
            case_data.get('J_subject_matter', ''),
            case_data.get('Q_case_details', '')
        ]
        
        combined_text = ' '.join(str(field) for field in text_fields).lower()
        
        # ç´§æ€¥æƒ…å†µæ£€æµ‹
        emergency_score = 0
        for keyword in self.emergency_keywords:
            if keyword.lower() in combined_text:
                emergency_score += 1
        
        # checkç´§æ€¥æ¡ˆä»¶classå‹
        for urgent_type in self.urgent_types:
            if urgent_type.lower() in combined_text:
                emergency_score += 0.5
        
        # checkä¸€èˆ¬æ¡ˆä»¶classå‹
        general_score = 0
        for general_type in self.general_types:
            if general_type.lower() in combined_text:
                general_score += 1
        
        # æ¥æºæƒé‡
        source = case_data.get('B_source', '').lower()
        if 'rcc' in source:
            emergency_score += 0.3  # RCCæ¡ˆä»¶é€šå¸¸æ›´ç´§æ€¥
        elif '1823' in source:
            emergency_score += 0.2  # 1823æŠ•è¯‰å¯èƒ½è¾ƒç´§æ€¥
        
        # å†³ç­–é€»è¾‘
        if emergency_score >= 2:
            return 'Emergency', min(0.9, 0.5 + emergency_score * 0.1)
        elif emergency_score >= 1 or (emergency_score > 0 and general_score == 0):
            return 'Urgent', min(0.8, 0.4 + emergency_score * 0.1)
        else:
            return 'General', min(0.7, 0.3 + general_score * 0.1)
    
    def train_ml_model(self) -> bool:
        """trainingæœºå™¨å­¦ä¹ model"""
        try:
            if self.historical_data is None or len(self.historical_data) == 0:
                print("âš ï¸ æ²¡æœ‰å†å²dataï¼Œæ— æ³•trainingMLmodel")
                return False
            
            df = self.historical_data.copy()
            
            # æ‰¾åˆ°classå‹åˆ—
            type_col = None
            for col in df.columns:
                if 'type' in col.lower() and ('emergency' in col.lower() or 'urgent' in col.lower()):
                    type_col = col
                    break
            
            if not type_col:
                print("âš ï¸ æœªæ‰¾åˆ°classå‹åˆ—")
                return False
            
            # å‡†å¤‡trainingdata
            nature_col = None
            for col in df.columns:
                if 'nature' in col.lower():
                    nature_col = col
                    break
            
            if not nature_col:
                print("âš ï¸ æœªæ‰¾åˆ°æŠ•è¯‰æ€§è´¨åˆ—")
                return False
            
            # cleanupdata
            df = df.dropna(subset=[nature_col, type_col])
            df[nature_col] = df[nature_col].fillna('')
            
            X = df[nature_col].astype(str)
            y = df[type_col]
            
            # æ–‡æœ¬vectoråŒ–
            self.vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2
            )
            
            X_vectorized = self.vectorizer.fit_transform(X)
            
            # trainingmodel
            X_train, X_test, y_train, y_test = train_test_split(
                X_vectorized, y, test_size=0.2, random_state=42, stratify=y
            )
            
            self.model = RandomForestClassifier(
                n_estimators=100,
                random_state=42,
                class_weight='balanced'
            )
            
            self.model.fit(X_train, y_train)
            
            # evaluatemodel
            y_pred = self.model.predict(X_test)
            print("âœ… MLmodeltrainingå®Œæˆ")
            print("\nmodelè¯„ä¼°:")
            print(classification_report(y_test, y_pred))
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ MLmodeltrainingfailed: {e}")
            return False
    
    def ml_classification(self, case_data: Dict) -> Tuple[str, float]:
        """åŸºäºæœºå™¨å­¦ä¹ çš„classify"""
        if self.model is None or self.vectorizer is None:
            return 'General', 0.3
        
        try:
            # å‡†å¤‡æ–‡æœ¬
            text_fields = [
                case_data.get('I_nature_of_request', ''),
                case_data.get('J_subject_matter', ''),
                case_data.get('Q_case_details', '')
            ]
            
            combined_text = ' '.join(str(field) for field in text_fields)
            
            # vectoråŒ–
            X = self.vectorizer.transform([combined_text])
            
            # prediction
            prediction = self.model.predict(X)[0]
            probabilities = self.model.predict_proba(X)[0]
            
            # getconfidence
            class_names = self.model.classes_
            pred_idx = np.where(class_names == prediction)[0][0]
            confidence = probabilities[pred_idx]
            
            return prediction, confidence
            
        except Exception as e:
            print(f"âš ï¸ MLclassifyfailed: {e}")
            return 'General', 0.3
    
    def classify_case_type(self, case_data: Dict) -> Dict:
        """ç»¼åˆclassifyæ¡ˆä»¶classå‹"""
        
        # è§„åˆ™classify
        rule_type, rule_confidence = self.rule_based_classification(case_data)
        
        # MLclassify
        ml_type, ml_confidence = self.ml_classification(case_data)
        
        # ç»¼åˆå†³ç­–
        if rule_confidence > 0.7:
            # é«˜confidenceè§„åˆ™classify
            final_type = rule_type
            final_confidence = rule_confidence
            method = 'rule_based'
        elif ml_confidence > 0.6:
            # é«˜confidenceMLclassify
            final_type = ml_type
            final_confidence = ml_confidence
            method = 'machine_learning'
        elif rule_confidence > ml_confidence:
            # è§„åˆ™classifyconfidenceæ›´é«˜
            final_type = rule_type
            final_confidence = rule_confidence
            method = 'rule_based'
        else:
            # MLclassifyconfidenceæ›´é«˜
            final_type = ml_type
            final_confidence = ml_confidence
            method = 'machine_learning'
        
        # securitycheckï¼šç¡®ä¿returnæœ‰æ•ˆclasså‹
        if final_type not in ['Emergency', 'Urgent', 'General']:
            final_type = 'General'
            final_confidence = 0.5
            method = 'default'
        
        return {
            'predicted_type': final_type,
            'confidence': final_confidence,
            'method': method,
            'rule_prediction': {'type': rule_type, 'confidence': rule_confidence},
            'ml_prediction': {'type': ml_type, 'confidence': ml_confidence},
            'type_code': {'Emergency': '1', 'Urgent': '2', 'General': '3'}[final_type]
        }
    
    def initialize(self) -> bool:
        """initializeclassifyå™¨ï¼Œä½¿ç”¨cacheä¼˜åŒ–"""
        print("ğŸš€ initializeSRRæ¡ˆä»¶classå‹AIclassifyå™¨...")
        
        # å°è¯•ä»cachegetå®Œæ•´çš„classifyå™¨
        cache_key = "srr_case_type_classifier"
        cached_classifier = get_cached_model(cache_key)
        
        if cached_classifier:
            # ä½¿ç”¨cacheçš„classifyå™¨
            self.model = cached_classifier.get('model')
            self.vectorizer = cached_classifier.get('vectorizer')
            self.label_encoder = cached_classifier.get('label_encoder')
            self.feature_names = cached_classifier.get('feature_names', [])
            self.classification_rules = cached_classifier.get('classification_rules', {})
            self.historical_data = cached_classifier.get('historical_data')
            print("âœ… ä½¿ç”¨cacheçš„AIclassifyå™¨ (è·³è¿‡training)")
            return True
        
        # cacheæœªå‘½ä¸­ï¼Œæ­£å¸¸initialize
        # loadå†å²data
        self.srr_data, self.complaints_data = load_training_data()
        
        # loadSRRè§„åˆ™
        self.rules_data = load_srr_rules()
        
        # trainingMLmodel
        ml_success = self.train_ml_model()
        
        # cacheclassifyå™¨
        try:
            classifier_cache = {
                'model': self.model,
                'vectorizer': self.vectorizer,
                'label_encoder': self.label_encoder,
                'feature_names': self.feature_names,
                'classification_rules': self.classification_rules,
                'historical_data': self.historical_data
            }
            cache_model(cache_key, classifier_cache)
        except Exception as e:
            print(f"âš ï¸ cacheclassifyå™¨failed: {e}")
        
        if ml_success:
            print("âœ… AIclassifyå™¨initializeå®Œæˆ (è§„åˆ™ + ML)")
        else:
            print("âœ… AIclassifyå™¨initializeå®Œæˆ (ä»…è§„åˆ™)")
        
        return True
    
    def get_classification_explanation(self, case_data: Dict, result: Dict) -> str:
        """è·å–classifyè§£é‡Š"""
        explanation_parts = []
        
        # åŸºæœ¬information
        explanation_parts.append(f"classifyresult: {result['predicted_type']}")
        explanation_parts.append(f"confidence: {result['confidence']:.2f}")
        explanation_parts.append(f"classifymethod: {result['method']}")
        
        # å…³keyå› ç´ 
        text_fields = [
            case_data.get('I_nature_of_request', ''),
            case_data.get('J_subject_matter', ''),
            case_data.get('Q_case_details', '')
        ]
        combined_text = ' '.join(str(field) for field in text_fields).lower()
        
        # æ£€æµ‹åˆ°çš„å…³keyè¯
        detected_keywords = []
        for keyword in self.emergency_keywords:
            if keyword.lower() in combined_text:
                detected_keywords.append(keyword)
        
        if detected_keywords:
            explanation_parts.append(f"æ£€æµ‹åˆ°ç´§æ€¥å…³é”®è¯: {', '.join(detected_keywords)}")
        
        # æ¥æºå½±å“
        source = case_data.get('B_source', '')
        if source:
            explanation_parts.append(f"æ¡ˆä»¶æ¥æº: {source}")
        
        return ' | '.join(explanation_parts)

# å…¨å±€classifyå™¨instance
_classifier_instance = None

def get_classifier() -> SRRCaseTypeClassifier:
    """è·å–classifyå™¨instance (å•ä¾‹æ¨¡å¼)"""
    global _classifier_instance
    if _classifier_instance is None:
        _classifier_instance = SRRCaseTypeClassifier()
        _classifier_instance.initialize()
    return _classifier_instance

def classify_case_type_ai(case_data: Dict) -> Dict:
    """AIclassifyæ¡ˆä»¶classå‹çš„ä¸»è¦æ¥å£"""
    try:
        classifier = get_classifier()
        result = classifier.classify_case_type(case_data)
        
        # æ·»åŠ è§£é‡Š
        explanation = classifier.get_classification_explanation(case_data, result)
        result['explanation'] = explanation
        
        print(f"ğŸ¤– AIclassifyresult: {result['predicted_type']} (confidence: {result['confidence']:.2f})")
        print(f"ğŸ“ classifyä¾æ®: {explanation}")
        
        return result
        
    except Exception as e:
        print(f"âš ï¸ AIclassifyfailed: {e}")
        # returné»˜è®¤classify
        return {
            'predicted_type': 'General',
            'confidence': 0.5,
            'method': 'default',
            'type_code': '3',
            'explanation': f'AIclassifyfailedï¼Œä½¿ç”¨é»˜è®¤classify: {e}'
        }

if __name__ == "__main__":
    # testclassifyå™¨
    print("ğŸ§ª æµ‹è¯•SRRæ¡ˆä»¶classå‹AIclassifyå™¨")
    
    # testæ¡ˆä¾‹
    test_cases = [
        {
            'I_nature_of_request': 'Fallen tree blocking road, immediate danger to public',
            'J_subject_matter': 'Emergency tree removal',
            'Q_case_details': 'Large tree fell across main road during storm, blocking traffic',
            'B_source': 'RCC',
            'G_slope_no': '11SW-D/R805'
        },
        {
            'I_nature_of_request': 'Grass cutting required for slope maintenance',
            'J_subject_matter': 'Routine maintenance',
            'Q_case_details': 'Regular grass cutting for slope upkeep',
            'B_source': 'ICC',
            'G_slope_no': '11SE-C/C805'
        },
        {
            'I_nature_of_request': 'Water seepage observed on slope',
            'J_subject_matter': 'Slope inspection required',
            'Q_case_details': 'Resident reported water seepage, needs investigation',
            'B_source': '1823',
            'G_slope_no': '11SW-B/F199'
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- æµ‹è¯•æ¡ˆä¾‹ {i} ---")
        result = classify_case_type_ai(test_case)
        print(f"predictionclasså‹: {result['predicted_type']}")
        print(f"classå‹ä»£ç : {result['type_code']}")
        print(f"è§£é‡Š: {result['explanation']}")
