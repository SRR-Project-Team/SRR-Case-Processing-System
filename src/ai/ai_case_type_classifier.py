"""
AIæ¡ˆä»¶ç±»å‹åˆ†ç±»å™¨æ¨¡å—

æœ¬æ¨¡å—å®ç°åŸºäºæœºå™¨å­¦ä¹ å’Œè§„åˆ™åŒ¹é…çš„æ™ºèƒ½æ¡ˆä»¶ç±»å‹åˆ†ç±»ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨å°†SRRæ¡ˆä»¶
åˆ†ç±»ä¸ºEmergencyï¼ˆç´§æ€¥ï¼‰ã€Urgentï¼ˆç´§æ€¥ï¼‰æˆ–Generalï¼ˆä¸€èˆ¬ï¼‰ä¸‰ç§ç±»å‹ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. åŸºäºå†å²æ¡ˆä»¶æ•°æ®è¿›è¡Œæœºå™¨å­¦ä¹ è®­ç»ƒ
2. ç»“åˆSRRè§„åˆ™æ–‡æ¡£è¿›è¡Œè§„åˆ™åŒ¹é…
3. ä½¿ç”¨æ··åˆæ–¹æ³•ï¼ˆML + è§„åˆ™ï¼‰æé«˜åˆ†ç±»å‡†ç¡®ç‡
4. æ”¯æŒæ¨¡å‹ç¼“å­˜å’Œå¢é‡å­¦ä¹ 
5. æä¾›è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Šå’Œç½®ä¿¡åº¦è¯„ä¼°

æŠ€æœ¯å®ç°ï¼š
- æœºå™¨å­¦ä¹ ï¼šRandomForestClassifier + TF-IDFå‘é‡åŒ–
- è§„åˆ™åŒ¹é…ï¼šåŸºäºå…³é”®è¯å’Œè¯­ä¹‰çš„è§„åˆ™å¼•æ“
- æ•°æ®æ¥æºï¼šSRRå†å²æ•°æ® + æŠ•è¯‰æ¡ˆä»¶æ•°æ® + SRRè§„åˆ™æ–‡æ¡£

ä½œè€…: Project3 Team
ç‰ˆæœ¬: 2.0
"""

import pandas as pd

def load_srr_rules():
    """
    åŠ è½½SRRè§„åˆ™æ–‡æ¡£æ•°æ®
    
    ä»é¢„å¤„ç†çš„JSONæ–‡ä»¶ä¸­åŠ è½½SRRè§„åˆ™å†…å®¹ï¼Œè¿™äº›è§„åˆ™ç”¨äºè§„åˆ™åŒ¹é…åˆ†ç±»ã€‚
    è§„åˆ™æ–‡ä»¶åŒ…å«ä»SRR rules.docxæ–‡æ¡£ä¸­æå–çš„å…³é”®è¯å’Œåˆ†ç±»æ ‡å‡†ã€‚
    
    Returns:
        dict: åŒ…å«è§„åˆ™å†…å®¹çš„å­—å…¸
        {
            'content': list,  # è§„åˆ™æ–‡æœ¬å†…å®¹åˆ—è¡¨
            'paragraphs': int  # æ®µè½æ•°é‡
        }
        
    Example:
        >>> rules = load_srr_rules()
        >>> print(f"åŠ è½½äº† {rules['paragraphs']} ä¸ªè§„åˆ™æ®µè½")
    """
    import json
    import os
    
    rules_file = 'models/config/srr_rules.json'
    if os.path.exists(rules_file):
        with open(rules_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        print("âš ï¸ SRRè§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨")
        return {'content': [], 'paragraphs': 0}


def load_training_data():
    """
    åŠ è½½æœºå™¨å­¦ä¹ è®­ç»ƒæ•°æ®
    
    ä»é¢„å¤„ç†çš„pickleæ–‡ä»¶ä¸­åŠ è½½å†å²æ¡ˆä»¶æ•°æ®ï¼Œç”¨äºè®­ç»ƒåˆ†ç±»æ¨¡å‹ã€‚
    æ•°æ®åŒ…å«SRRæ¡ˆä»¶æ•°æ®å’ŒæŠ•è¯‰æ¡ˆä»¶æ•°æ®ï¼Œå·²è¿›è¡Œæ¸…æ´—å’Œé¢„å¤„ç†ã€‚
    
    Returns:
        tuple: (srr_data, complaints_data)
            - srr_data (list): SRRæ¡ˆä»¶æ•°æ®åˆ—è¡¨
            - complaints_data (list): æŠ•è¯‰æ¡ˆä»¶æ•°æ®åˆ—è¡¨
            
    Example:
        >>> srr_data, complaints_data = load_training_data()
        >>> print(f"SRRæ•°æ®: {len(srr_data)}æ¡, æŠ•è¯‰æ•°æ®: {len(complaints_data)}æ¡")
    """
    import pickle
    import os
    
    data_file = 'models/ai_models/training_data.pkl'
    if os.path.exists(data_file):
        with open(data_file, 'rb') as f:
            data = pickle.load(f)
        return data.get('srr_data', []), data.get('complaints_data', [])
    else:
        print("âš ï¸ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        return [], []

import numpy as np
import re
from typing import Dict, List, Optional, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import pickle
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')
from .ai_model_cache import get_cached_model, cache_model

class SRRCaseTypeClassifier:
    """SRRæ¡ˆä»¶ç±»å‹AIåˆ†ç±»å™¨"""
    
    def __init__(self, data_path: str = "models"):
        self.data_path = data_path
        self.model = None
        self.vectorizer = None
        self.label_encoder = None
        self.feature_names = []
        self.classification_rules = {}
        self.historical_data = None
        
        # ç´§æ€¥å…³é”®è¯ (Emergency indicators)
        self.emergency_keywords = [
            'collapse', 'collapsed', 'falling', 'fallen', 'immediate danger', 
            'urgent repair', 'safety risk', 'hazard', 'emergency', 'critical',
            'å€’å¡Œ', 'å´©å¡Œ', 'ç·Šæ€¥', 'å±éšª', 'ç«‹å³', 'å®‰å…¨é¢¨éšª', 'åš´é‡', 'ç·Šæ€¥ä¿®å¾©'
        ]
        
        # ç´§æ€¥æ¡ˆä»¶ç±»å‹
        self.urgent_types = [
            'hazardous tree', 'fallen tree', 'drainage blockage', 'water seepage',
            'remove debris', 'safety concern', 'structural damage'
        ]
        
        # ä¸€èˆ¬æ¡ˆä»¶ç±»å‹  
        self.general_types = [
            'grass cutting', 'tree trimming', 'pruning', 'maintenance',
            'routine inspection', 'general enquiry', 'information request'
        ]
        
    def load_historical_data(self) -> pd.DataFrame:
        """åŠ è½½å†å²æ•°æ®"""
        try:
            # åŠ è½½SRRå†å²æ•°æ®
            csv_path = os.path.join(self.data_path, "SRR data 2021-2024.csv")
            df = pd.read_csv(csv_path, encoding='latin1')
            
            print(f"âœ… åŠ è½½å†å²æ•°æ®æˆåŠŸ: {len(df)} æ¡è®°å½•")
            
            # æ¸…ç†åˆ—å
            df.columns = [col.strip().replace('\n', ' ') for col in df.columns]
            
            # æ‰¾åˆ°ç±»å‹åˆ—
            type_col = None
            for col in df.columns:
                if 'type' in col.lower() and ('emergency' in col.lower() or 'urgent' in col.lower()):
                    type_col = col
                    break
            
            if type_col:
                # æ¸…ç†ç±»å‹æ•°æ®
                df[type_col] = df[type_col].fillna('General')
                df = df[df[type_col].isin(['Emergency', 'Urgent', 'General'])]
                
                print(f"ç±»å‹åˆ†å¸ƒ:")
                print(df[type_col].value_counts())
                
            self.historical_data = df
            return df
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å†å²æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def load_srr_rules(self) -> Dict:
        """åŠ è½½SRRè§„åˆ™"""
        try:
            from docx import Document
            
            rules_path = os.path.join(self.data_path, "SRR rules.docx")
            doc = Document(rules_path)
            
            rules = {
                'emergency_criteria': [],
                'urgent_criteria': [],
                'general_criteria': []
            }
            
            current_section = None
            for para in doc.paragraphs:
                text = para.text.strip().lower()
                if not text:
                    continue
                    
                # è¯†åˆ«è§„åˆ™éƒ¨åˆ†
                if 'emergency' in text:
                    current_section = 'emergency_criteria'
                elif 'urgent' in text:
                    current_section = 'urgent_criteria'
                elif 'general' in text:
                    current_section = 'general_criteria'
                elif current_section and len(text) > 10:
                    rules[current_section].append(text)
            
            print(f"âœ… åŠ è½½SRRè§„åˆ™æˆåŠŸ")
            for section, items in rules.items():
                print(f"{section}: {len(items)} æ¡è§„åˆ™")
                
            self.classification_rules = rules
            return rules
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½SRRè§„åˆ™å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤è§„åˆ™
            return self._get_default_rules()
    
    def _get_default_rules(self) -> Dict:
        """è·å–é»˜è®¤åˆ†ç±»è§„åˆ™"""
        return {
            'emergency_criteria': [
                'immediate safety risk to public',
                'structural collapse or imminent collapse',
                'fallen tree blocking road or causing danger',
                'severe water seepage causing instability',
                'major drainage blockage causing flooding'
            ],
            'urgent_criteria': [
                'hazardous tree requiring immediate attention',
                'water seepage requiring prompt investigation',
                'drainage blockage affecting slope stability',
                'debris removal for safety reasons',
                'slope maintenance affecting public safety'
            ],
            'general_criteria': [
                'routine grass cutting and maintenance',
                'general tree trimming and pruning',
                'regular slope inspection',
                'information enquiry',
                'non-urgent maintenance work'
            ]
        }
    
    def extract_features(self, case_data: Dict) -> Dict:
        """ä»æ¡ˆä»¶æ•°æ®ä¸­æå–ç‰¹å¾"""
        features = {}
        
        # æ–‡æœ¬ç‰¹å¾
        text_fields = [
            case_data.get('I_nature_of_request', ''),
            case_data.get('J_subject_matter', ''),
            case_data.get('Q_case_details', ''),
            case_data.get('B_source', '')
        ]
        
        combined_text = ' '.join(str(field) for field in text_fields).lower()
        
        # ç´§æ€¥å…³é”®è¯è®¡æ•°
        emergency_count = sum(1 for keyword in self.emergency_keywords 
                            if keyword.lower() in combined_text)
        features['emergency_keywords'] = emergency_count
        
        # æ¡ˆä»¶æ¥æºç‰¹å¾
        source = case_data.get('B_source', '').lower()
        features['source_rcc'] = 1 if 'rcc' in source else 0
        features['source_icc'] = 1 if 'icc' in source else 0
        features['source_1823'] = 1 if '1823' in source else 0
        
        # æ—¶é—´ç‰¹å¾ (å‘¨æœ«/èŠ‚å‡æ—¥å¯èƒ½æ›´ç´§æ€¥)
        try:
            date_str = case_data.get('A_date_received', '')
            if date_str:
                # ç®€å•çš„æ—¶é—´ç‰¹å¾
                features['is_weekend'] = 0  # å¯ä»¥æ ¹æ®å®é™…æ—¥æœŸè®¡ç®—
        except:
            features['is_weekend'] = 0
        
        # æ–œå¡ç¼–å·ç‰¹å¾ (æŸäº›åŒºåŸŸå¯èƒ½æ›´å®¹æ˜“æœ‰ç´§æ€¥æƒ…å†µ)
        slope_no = case_data.get('G_slope_no', '')
        features['has_slope_no'] = 1 if slope_no else 0
        
        # è”ç³»ä¿¡æ¯ç‰¹å¾ (æœ‰è”ç³»æ–¹å¼å¯èƒ½æ›´ç´§æ€¥)
        contact = case_data.get('F_contact_no', '')
        features['has_contact'] = 1 if contact else 0
        
        # æ–‡æœ¬é•¿åº¦ç‰¹å¾ (è¯¦ç»†æè¿°å¯èƒ½è¡¨ç¤ºæ›´å¤æ‚çš„é—®é¢˜)
        features['text_length'] = len(combined_text)
        features['word_count'] = len(combined_text.split())
        
        return features
    
    def rule_based_classification(self, case_data: Dict) -> Tuple[str, float]:
        """åŸºäºè§„åˆ™çš„åˆ†ç±»"""
        text_fields = [
            case_data.get('I_nature_of_request', ''),
            case_data.get('J_subject_matter', ''),
            case_data.get('Q_case_details', '')
        ]
        
        combined_text = ' '.join(str(field) for field in text_fields).lower()
        
        # ç´§æ€¥æƒ…å†µæ£€æµ‹
        emergency_score = 0
        for keyword in self.emergency_keywords:
            if keyword.lower() in combined_text:
                emergency_score += 1
        
        # æ£€æŸ¥ç´§æ€¥æ¡ˆä»¶ç±»å‹
        for urgent_type in self.urgent_types:
            if urgent_type.lower() in combined_text:
                emergency_score += 0.5
        
        # æ£€æŸ¥ä¸€èˆ¬æ¡ˆä»¶ç±»å‹
        general_score = 0
        for general_type in self.general_types:
            if general_type.lower() in combined_text:
                general_score += 1
        
        # æ¥æºæƒé‡
        source = case_data.get('B_source', '').lower()
        if 'rcc' in source:
            emergency_score += 0.3  # RCCæ¡ˆä»¶é€šå¸¸æ›´ç´§æ€¥
        elif '1823' in source:
            emergency_score += 0.2  # 1823æŠ•è¯‰å¯èƒ½è¾ƒç´§æ€¥
        
        # å†³ç­–é€»è¾‘
        if emergency_score >= 2:
            return 'Emergency', min(0.9, 0.5 + emergency_score * 0.1)
        elif emergency_score >= 1 or (emergency_score > 0 and general_score == 0):
            return 'Urgent', min(0.8, 0.4 + emergency_score * 0.1)
        else:
            return 'General', min(0.7, 0.3 + general_score * 0.1)
    
    def train_ml_model(self) -> bool:
        """è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹"""
        try:
            if self.historical_data is None or len(self.historical_data) == 0:
                print("âš ï¸ æ²¡æœ‰å†å²æ•°æ®ï¼Œæ— æ³•è®­ç»ƒMLæ¨¡å‹")
                return False
            
            df = self.historical_data.copy()
            
            # æ‰¾åˆ°ç±»å‹åˆ—
            type_col = None
            for col in df.columns:
                if 'type' in col.lower() and ('emergency' in col.lower() or 'urgent' in col.lower()):
                    type_col = col
                    break
            
            if not type_col:
                print("âš ï¸ æœªæ‰¾åˆ°ç±»å‹åˆ—")
                return False
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            nature_col = None
            for col in df.columns:
                if 'nature' in col.lower():
                    nature_col = col
                    break
            
            if not nature_col:
                print("âš ï¸ æœªæ‰¾åˆ°æŠ•è¯‰æ€§è´¨åˆ—")
                return False
            
            # æ¸…ç†æ•°æ®
            df = df.dropna(subset=[nature_col, type_col])
            df[nature_col] = df[nature_col].fillna('')
            
            X = df[nature_col].astype(str)
            y = df[type_col]
            
            # æ–‡æœ¬å‘é‡åŒ–
            self.vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2
            )
            
            X_vectorized = self.vectorizer.fit_transform(X)
            
            # è®­ç»ƒæ¨¡å‹
            X_train, X_test, y_train, y_test = train_test_split(
                X_vectorized, y, test_size=0.2, random_state=42, stratify=y
            )
            
            self.model = RandomForestClassifier(
                n_estimators=100,
                random_state=42,
                class_weight='balanced'
            )
            
            self.model.fit(X_train, y_train)
            
            # è¯„ä¼°æ¨¡å‹
            y_pred = self.model.predict(X_test)
            print("âœ… MLæ¨¡å‹è®­ç»ƒå®Œæˆ")
            print("\næ¨¡å‹è¯„ä¼°:")
            print(classification_report(y_test, y_pred))
            
            return True
            
        except Exception as e:
            print(f"âš ï¸ MLæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def ml_classification(self, case_data: Dict) -> Tuple[str, float]:
        """åŸºäºæœºå™¨å­¦ä¹ çš„åˆ†ç±»"""
        if self.model is None or self.vectorizer is None:
            return 'General', 0.3
        
        try:
            # å‡†å¤‡æ–‡æœ¬
            text_fields = [
                case_data.get('I_nature_of_request', ''),
                case_data.get('J_subject_matter', ''),
                case_data.get('Q_case_details', '')
            ]
            
            combined_text = ' '.join(str(field) for field in text_fields)
            
            # å‘é‡åŒ–
            X = self.vectorizer.transform([combined_text])
            
            # é¢„æµ‹
            prediction = self.model.predict(X)[0]
            probabilities = self.model.predict_proba(X)[0]
            
            # è·å–ç½®ä¿¡åº¦
            class_names = self.model.classes_
            pred_idx = np.where(class_names == prediction)[0][0]
            confidence = probabilities[pred_idx]
            
            return prediction, confidence
            
        except Exception as e:
            print(f"âš ï¸ MLåˆ†ç±»å¤±è´¥: {e}")
            return 'General', 0.3
    
    def classify_case_type(self, case_data: Dict) -> Dict:
        """ç»¼åˆåˆ†ç±»æ¡ˆä»¶ç±»å‹"""
        
        # è§„åˆ™åˆ†ç±»
        rule_type, rule_confidence = self.rule_based_classification(case_data)
        
        # MLåˆ†ç±»
        ml_type, ml_confidence = self.ml_classification(case_data)
        
        # ç»¼åˆå†³ç­–
        if rule_confidence > 0.7:
            # é«˜ç½®ä¿¡åº¦è§„åˆ™åˆ†ç±»
            final_type = rule_type
            final_confidence = rule_confidence
            method = 'rule_based'
        elif ml_confidence > 0.6:
            # é«˜ç½®ä¿¡åº¦MLåˆ†ç±»
            final_type = ml_type
            final_confidence = ml_confidence
            method = 'machine_learning'
        elif rule_confidence > ml_confidence:
            # è§„åˆ™åˆ†ç±»ç½®ä¿¡åº¦æ›´é«˜
            final_type = rule_type
            final_confidence = rule_confidence
            method = 'rule_based'
        else:
            # MLåˆ†ç±»ç½®ä¿¡åº¦æ›´é«˜
            final_type = ml_type
            final_confidence = ml_confidence
            method = 'machine_learning'
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è¿”å›æœ‰æ•ˆç±»å‹
        if final_type not in ['Emergency', 'Urgent', 'General']:
            final_type = 'General'
            final_confidence = 0.5
            method = 'default'
        
        return {
            'predicted_type': final_type,
            'confidence': final_confidence,
            'method': method,
            'rule_prediction': {'type': rule_type, 'confidence': rule_confidence},
            'ml_prediction': {'type': ml_type, 'confidence': ml_confidence},
            'type_code': {'Emergency': '1', 'Urgent': '2', 'General': '3'}[final_type]
        }
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–åˆ†ç±»å™¨ï¼Œä½¿ç”¨ç¼“å­˜ä¼˜åŒ–"""
        print("ğŸš€ åˆå§‹åŒ–SRRæ¡ˆä»¶ç±»å‹AIåˆ†ç±»å™¨...")
        
        # å°è¯•ä»ç¼“å­˜è·å–å®Œæ•´çš„åˆ†ç±»å™¨
        cache_key = "srr_case_type_classifier"
        cached_classifier = get_cached_model(cache_key)
        
        if cached_classifier:
            # ä½¿ç”¨ç¼“å­˜çš„åˆ†ç±»å™¨
            self.model = cached_classifier.get('model')
            self.vectorizer = cached_classifier.get('vectorizer')
            self.label_encoder = cached_classifier.get('label_encoder')
            self.feature_names = cached_classifier.get('feature_names', [])
            self.classification_rules = cached_classifier.get('classification_rules', {})
            self.historical_data = cached_classifier.get('historical_data')
            print("âœ… ä½¿ç”¨ç¼“å­˜çš„AIåˆ†ç±»å™¨ (è·³è¿‡è®­ç»ƒ)")
            return True
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ­£å¸¸åˆå§‹åŒ–
        # åŠ è½½å†å²æ•°æ®
        self.srr_data, self.complaints_data = load_training_data()
        
        # åŠ è½½SRRè§„åˆ™
        self.rules_data = load_srr_rules()
        
        # è®­ç»ƒMLæ¨¡å‹
        ml_success = self.train_ml_model()
        
        # ç¼“å­˜åˆ†ç±»å™¨
        try:
            classifier_cache = {
                'model': self.model,
                'vectorizer': self.vectorizer,
                'label_encoder': self.label_encoder,
                'feature_names': self.feature_names,
                'classification_rules': self.classification_rules,
                'historical_data': self.historical_data
            }
            cache_model(cache_key, classifier_cache)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜åˆ†ç±»å™¨å¤±è´¥: {e}")
        
        if ml_success:
            print("âœ… AIåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ (è§„åˆ™ + ML)")
        else:
            print("âœ… AIåˆ†ç±»å™¨åˆå§‹åŒ–å®Œæˆ (ä»…è§„åˆ™)")
        
        return True
    
    def get_classification_explanation(self, case_data: Dict, result: Dict) -> str:
        """è·å–åˆ†ç±»è§£é‡Š"""
        explanation_parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        explanation_parts.append(f"åˆ†ç±»ç»“æœ: {result['predicted_type']}")
        explanation_parts.append(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
        explanation_parts.append(f"åˆ†ç±»æ–¹æ³•: {result['method']}")
        
        # å…³é”®å› ç´ 
        text_fields = [
            case_data.get('I_nature_of_request', ''),
            case_data.get('J_subject_matter', ''),
            case_data.get('Q_case_details', '')
        ]
        combined_text = ' '.join(str(field) for field in text_fields).lower()
        
        # æ£€æµ‹åˆ°çš„å…³é”®è¯
        detected_keywords = []
        for keyword in self.emergency_keywords:
            if keyword.lower() in combined_text:
                detected_keywords.append(keyword)
        
        if detected_keywords:
            explanation_parts.append(f"æ£€æµ‹åˆ°ç´§æ€¥å…³é”®è¯: {', '.join(detected_keywords)}")
        
        # æ¥æºå½±å“
        source = case_data.get('B_source', '')
        if source:
            explanation_parts.append(f"æ¡ˆä»¶æ¥æº: {source}")
        
        return ' | '.join(explanation_parts)

# å…¨å±€åˆ†ç±»å™¨å®ä¾‹
_classifier_instance = None

def get_classifier() -> SRRCaseTypeClassifier:
    """è·å–åˆ†ç±»å™¨å®ä¾‹ (å•ä¾‹æ¨¡å¼)"""
    global _classifier_instance
    if _classifier_instance is None:
        _classifier_instance = SRRCaseTypeClassifier()
        _classifier_instance.initialize()
    return _classifier_instance

def classify_case_type_ai(case_data: Dict) -> Dict:
    """AIåˆ†ç±»æ¡ˆä»¶ç±»å‹çš„ä¸»è¦æ¥å£"""
    try:
        classifier = get_classifier()
        result = classifier.classify_case_type(case_data)
        
        # æ·»åŠ è§£é‡Š
        explanation = classifier.get_classification_explanation(case_data, result)
        result['explanation'] = explanation
        
        print(f"ğŸ¤– AIåˆ†ç±»ç»“æœ: {result['predicted_type']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
        print(f"ğŸ“ åˆ†ç±»ä¾æ®: {explanation}")
        
        return result
        
    except Exception as e:
        print(f"âš ï¸ AIåˆ†ç±»å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤åˆ†ç±»
        return {
            'predicted_type': 'General',
            'confidence': 0.5,
            'method': 'default',
            'type_code': '3',
            'explanation': f'AIåˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»: {e}'
        }

if __name__ == "__main__":
    # æµ‹è¯•åˆ†ç±»å™¨
    print("ğŸ§ª æµ‹è¯•SRRæ¡ˆä»¶ç±»å‹AIåˆ†ç±»å™¨")
    
    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {
            'I_nature_of_request': 'Fallen tree blocking road, immediate danger to public',
            'J_subject_matter': 'Emergency tree removal',
            'Q_case_details': 'Large tree fell across main road during storm, blocking traffic',
            'B_source': 'RCC',
            'G_slope_no': '11SW-D/R805'
        },
        {
            'I_nature_of_request': 'Grass cutting required for slope maintenance',
            'J_subject_matter': 'Routine maintenance',
            'Q_case_details': 'Regular grass cutting for slope upkeep',
            'B_source': 'ICC',
            'G_slope_no': '11SE-C/C805'
        },
        {
            'I_nature_of_request': 'Water seepage observed on slope',
            'J_subject_matter': 'Slope inspection required',
            'Q_case_details': 'Resident reported water seepage, needs investigation',
            'B_source': '1823',
            'G_slope_no': '11SW-B/F199'
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n--- æµ‹è¯•æ¡ˆä¾‹ {i} ---")
        result = classify_case_type_ai(test_case)
        print(f"é¢„æµ‹ç±»å‹: {result['predicted_type']}")
        print(f"ç±»å‹ä»£ç : {result['type_code']}")
        print(f"è§£é‡Š: {result['explanation']}")
