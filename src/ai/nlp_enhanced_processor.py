"""
NLPå¢å¼ºå¤„ç†å™¨
ä½¿ç”¨transformerã€BERTç­‰æŠ€æœ¯è¿›è¡Œæ™ºèƒ½æ–‡æœ¬æ€»ç»“å’Œå†…å®¹åˆ†æ
"""

import re
import os
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings('ignore')

class NLPEnhancedProcessor:
    """NLPå¢å¼ºå¤„ç†å™¨"""
    
    def __init__(self):
        self.summarizer = None
        self._init_nlp_models()
    
    def _init_nlp_models(self):
        """åˆå§‹åŒ–NLPæ¨¡å‹"""
        # ç›´æ¥ä½¿ç”¨è§„åˆ™åŸºç¡€æ–¹æ³•ï¼Œé¿å…transformeræ¨¡å‹çš„æ€§èƒ½é—®é¢˜
        print("âœ… ä½¿ç”¨ä¼˜åŒ–çš„è§„åˆ™åŸºç¡€NLPå¤„ç†")
        self.summarizer = None
    
    def extract_nature_of_request(self, txt_content: str, email_content: str = None) -> str:
        """
        ä½¿ç”¨NLPæŠ€æœ¯æå–å’Œæ€»ç»“Nature of Request
        
        Args:
            txt_content (str): TXTæ–‡ä»¶å†…å®¹
            email_content (str): é‚®ä»¶å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            
        Returns:
            str: æ™ºèƒ½æ€»ç»“çš„è¯‰æ±‚å†…å®¹
        """
        # ä¼˜å…ˆä½¿ç”¨é‚®ä»¶å†…å®¹è¿›è¡Œæ€»ç»“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if email_content and email_content.strip():
            print("ğŸ“§ ä¼˜å…ˆä½¿ç”¨é‚®ä»¶å†…å®¹è¿›è¡Œæ€»ç»“")
            email_summary = self._rule_based_summarize_email(email_content)
            if email_summary and email_summary.strip():
                print("âœ… é‚®ä»¶å†…å®¹æ€»ç»“æˆåŠŸ")
                return email_summary
        
        # å¦‚æœé‚®ä»¶å†…å®¹æ— æ³•æ€»ç»“ï¼Œä½¿ç”¨TXTå†…å®¹
        source_content = txt_content if txt_content and txt_content.strip() else ""
        
        if not source_content.strip():
            return "æ— æ³•æå–è¯‰æ±‚å†…å®¹"
        
        print("ğŸ” ä½¿ç”¨ä¼˜åŒ–çš„NLPæŠ€æœ¯åˆ†æè¯‰æ±‚å†…å®¹...")
        
        # ç›´æ¥ä½¿ç”¨ä¼˜åŒ–çš„è§„åˆ™åŸºç¡€æ–¹æ³•ï¼ˆæ›´å¿«æ›´å‡†ç¡®ï¼‰
        rule_summary = self._rule_based_summarize(source_content)
        if rule_summary and rule_summary.strip():
            print("âœ… ä¼˜åŒ–è§„åˆ™æ€»ç»“æˆåŠŸ")
            return rule_summary
        
        # å¦‚æœè§„åˆ™æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä»é‚®ä»¶å†…å®¹ä¸­æå–
        if email_content and email_content.strip():
            email_summary = self._rule_based_summarize(email_content)
            if email_summary and email_summary.strip():
                print("âœ… é‚®ä»¶å†…å®¹æ€»ç»“æˆåŠŸ")
                return email_summary
        
        # å¤‡é€‰ï¼šä½¿ç”¨å…³é”®è¯æå–
        keyword_summary = self._keyword_based_summarize(source_content)
        if keyword_summary and keyword_summary.strip():
            print("âœ… å…³é”®è¯æ€»ç»“æˆåŠŸ")
            return keyword_summary
        
        # æœ€åå¤‡é€‰ï¼šç®€å•æ–‡æœ¬æˆªå–
        if len(source_content) > 50:
            return source_content[:50] + "..."
        
        return "æ— æ³•æå–æœ‰æ•ˆä¿¡æ¯"
    
    def _transformer_summarize(self, content: str) -> Optional[str]:
        """ä½¿ç”¨transformeræ¨¡å‹è¿›è¡Œæ€»ç»“"""
        if not self.summarizer:
            return None
        
        try:
            # æ¸…ç†å’Œé¢„å¤„ç†æ–‡æœ¬
            cleaned_content = self._preprocess_text(content)
            
            # é™åˆ¶è¾“å…¥é•¿åº¦ï¼ˆtransformeræ¨¡å‹æœ‰è¾“å…¥é•¿åº¦é™åˆ¶ï¼‰
            max_length = 1000
            if len(cleaned_content) > max_length:
                cleaned_content = cleaned_content[:max_length]
            
            # ä½¿ç”¨transformeræ¨¡å‹æ€»ç»“
            summary = self.summarizer(cleaned_content, max_length=100, min_length=20, do_sample=False)
            
            if summary and len(summary) > 0:
                return summary[0]['summary_text']
            
        except Exception as e:
            print(f"Transformeræ€»ç»“å¼‚å¸¸: {e}")
        
        return None
    
    def _rule_based_summarize_email(self, email_content: str) -> Optional[str]:
        """
        ä¸“é—¨å¤„ç†é‚®ä»¶å†…å®¹çš„æ€»ç»“æ–¹æ³•
        """
        try:
            # ä»é‚®ä»¶å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯
            summary_parts = []
            
            # 1. æå–æ¡ˆä»¶ç¼–å·
            case_match = re.search(r'<CASE>:\s*([^\n]+)', email_content)
            if case_match:
                case_no = case_match.group(1).strip()
                summary_parts.append(f"æ¡ˆä»¶ç¼–å·: {case_no}")
            
            # 2. æå–éƒ¨é—¨ä¿¡æ¯
            dept_match = re.search(r'<DEPT>:\s*([^\n]+)', email_content)
            if dept_match:
                dept = dept_match.group(1).strip()
                summary_parts.append(f"è´Ÿè´£éƒ¨é—¨: {dept}")
            
            # 3. æå–æ”¶ä»¶äººä¿¡æ¯
            to_match = re.search(r'To:\s*-\s*([^\n,]+)', email_content)
            if to_match:
                to_dept = to_match.group(1).strip()
                summary_parts.append(f"è½¬ä»‹è‡³: {to_dept}")
            
            # 4. è¯†åˆ«æŸ¥è¯¢ç±»å‹
            if 'enquiry' in email_content.lower():
                summary_parts.append("æŸ¥è¯¢è¯·æ±‚")
            
            if 'referral' in email_content.lower():
                summary_parts.append("è½¬ä»‹å¤„ç†")
            
            # 5. æå–æ—¶é—´è¦æ±‚
            if '10 calendar days' in email_content:
                summary_parts.append("10å¤©å›å¤è¦æ±‚")
            
            if '21 calendar days' in email_content:
                summary_parts.append("21å¤©æœ€ç»ˆå›å¤è¦æ±‚")
            
            if summary_parts:
                return " | ".join(summary_parts)
            
        except Exception as e:
            print(f"é‚®ä»¶å†…å®¹æ€»ç»“å¼‚å¸¸: {e}")
        
        return None
    
    def _rule_based_summarize(self, content: str) -> Optional[str]:
        """ä¼˜åŒ–çš„è§„åˆ™åŸºç¡€æ€»ç»“æ–¹æ³•"""
        try:
            # å¿«é€Ÿæå–å…³é”®ä¿¡æ¯
            summary_parts = []
            
            # 1. æå–ä¸»æ—¨ä¿¡æ¯ï¼ˆæœ€é‡è¦ï¼‰
            subject_match = re.search(r'ä¸»æ—¨[ï¼š:]\s*([^\n]+)', content)
            if subject_match:
                subject = subject_match.group(1).strip()
                # æˆªå–å…³é”®éƒ¨åˆ†ï¼Œé¿å…è¿‡é•¿
                if len(subject) > 80:
                    subject = subject[:80] + "..."
                if 'æŸ¥è©¢' in subject and 'æ–œå¡' in subject:
                    summary_parts.append(f"æŸ¥è¯¢æ–œå¡ç»´ä¿®è¿›åº¦: {subject}")
                elif 'æ–œå¡' in subject:
                    summary_parts.append(f"æ–œå¡ç›¸å…³é—®é¢˜: {subject}")
                else:
                    summary_parts.append(f"ä¸»é¢˜: {subject}")
            
            # 1.1 æå–Subject Matterä¿¡æ¯
            subject_matter_match = re.search(r'Subject Matter[ï¼š:]\s*([^\n]+)', content)
            if subject_matter_match:
                subject_matter = subject_matter_match.group(1).strip()
                summary_parts.append(f"äº‹é¡¹: {subject_matter}")
            
            # 1.2 æå–Descriptionä¿¡æ¯
            desc_match = re.search(r'Description[ï¼š:]\s*([^\n]+)', content)
            if desc_match:
                description = desc_match.group(1).strip()
                summary_parts.append(f"æè¿°: {description}")
            
            # 2. æå–è¯·æ±‚ç±»å‹
            request_match = re.search(r'Request Type[ï¼š:]\s*([^\n]+)', content)
            if request_match:
                request_type = request_match.group(1).strip()
                summary_parts.append(f"è¯·æ±‚ç±»å‹: {request_type}")
            
            # 3. æå–æ–œå¡ç¼–å·
            slope_match = re.search(r'æ–œå¡ç·¨è™Ÿ([^\s]+)|11SW[-\w/]+', content)
            if slope_match:
                slope = slope_match.group(0).strip()
                summary_parts.append(f"æ¶‰åŠæ–œå¡: {slope}")
            
            # 4. æå–ä¸»è¦å…³æ³¨ç‚¹
            if 'é€²åº¦' in content:
                summary_parts.append("å…³æ³¨å·¥ç¨‹è¿›åº¦")
            elif 'ç¶­ä¿®' in content:
                summary_parts.append("å…³æ³¨ç»´ä¿®æƒ…å†µ")
            elif 'æŸ¥è©¢' in content:
                summary_parts.append("æŸ¥è¯¢è¯·æ±‚")
            
            # 5. æ„å»ºæœ€ç»ˆæ€»ç»“
            if summary_parts:
                return " | ".join(summary_parts)
            
            # 6. å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®ä¿¡æ¯ï¼Œæå–å…³é”®å¥å­
            lines = content.split('\\n')
            key_lines = []
            for line in lines:
                line = line.strip()
                if len(line) > 10 and any(keyword in line for keyword in ['æŸ¥è©¢', 'æ–œå¡', 'ç¶­ä¿®', 'é€²åº¦', '11SW']):
                    # æˆªå–å…³é”®éƒ¨åˆ†ï¼Œé¿å…è¿‡é•¿
                    if len(line) > 100:
                        line = line[:100] + "..."
                    key_lines.append(line)
                    if len(key_lines) >= 2:  # æœ€å¤šå–2è¡Œ
                        break
            
            if key_lines:
                return " | ".join(key_lines)
            
            # 7. æœ€åå¤‡é€‰ï¼šç®€å•æ–‡æœ¬æˆªå–
            if len(content) > 100:
                return content[:100] + "..."
            
        except Exception as e:
            print(f"è§„åˆ™åŸºç¡€æ€»ç»“å¼‚å¸¸: {e}")
        
        return None
    
    def _keyword_based_summarize(self, content: str) -> Optional[str]:
        """åŸºäºå…³é”®è¯çš„æ€»ç»“æ–¹æ³•"""
        try:
            # æå–å…³é”®è¯å’ŒçŸ­è¯­
            keywords = self._extract_keywords(content)
            
            if keywords:
                # æ„å»ºå…³é”®è¯æ€»ç»“
                return f"å…³é”®è¯æ€»ç»“: {', '.join(keywords[:5])}"
            
        except Exception as e:
            print(f"å…³é”®è¯æ€»ç»“å¼‚å¸¸: {e}")
        
        return None
    
    def _preprocess_text(self, content: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        content = re.sub(r'[^\w\s\u4e00-\u9fff.,!?]', ' ', content)
        
        # ç§»é™¤è¿‡çŸ­çš„è¡Œ
        lines = content.split('\n')
        filtered_lines = [line.strip() for line in lines if len(line.strip()) > 10]
        
        return ' '.join(filtered_lines)
    
    def _extract_key_information(self, content: str) -> Dict[str, str]:
        """æå–å…³é”®ä¿¡æ¯ - ä¸“æ³¨äºå®é™…è¯‰æ±‚å†…å®¹"""
        key_info = {}
        
        # æå–ä¸»é¢˜ - ä¸“æ³¨äºå®é™…è¯‰æ±‚
        subject_patterns = [
            r'ä¸»æ—¨[ï¼š:]\s*([^\n]+)',
            r'Subject Matter[ï¼š:]\s*([^\n]+)',
            r'æŸ¥è©¢æ–œå¡ç¶­ä¿®ç·¨è™Ÿ([^\n]+)ç¶­ä¿®å·¥ç¨‹é€²åº¦',
            r'æ–œå¡äº‹é …[^\n]*',
            r'æŸ¥è©¢[^\n]*æ–œå¡[^\n]*',
        ]
        
        for pattern in subject_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                subject = match.group(1).strip() if match.groups() else match.group(0).strip()
                key_info['subject'] = subject
                break
        
        # æå–è¯·æ±‚ç±»å‹
        request_patterns = [
            r'Request Type[ï¼š:]\s*([^\n]+)',
            r'è«‹æ±‚é¡å‹[ï¼š:]\s*([^\n]+)',
        ]
        
        for pattern in request_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                key_info['request_type'] = match.group(1).strip()
                break
        
        # æå–æ–œå¡ä¿¡æ¯ - æ›´ç²¾ç¡®çš„åŒ¹é…
        slope_patterns = [
            r'æ–œå¡ç·¨è™Ÿ([^\s]+)',
            r'11SW[-\w/]+',
            r'ç¶­ä¿®ç·¨è™Ÿ([^\s]+)',
            r'æ–œå¡[^\n]*ç·¨è™Ÿ[^\n]*',
        ]
        
        for pattern in slope_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                slope = match.group(1).strip() if match.groups() else match.group(0).strip()
                key_info['slope_info'] = slope
                break
        
        # æå–ä¸»è¦å…³æ³¨ç‚¹ - ä¸“æ³¨äºå®é™…è¯‰æ±‚
        concern_patterns = [
            r'æŸ¥è©¢[^\n]*é€²åº¦[^\n]*',
            r'ç¶­ä¿®[^\n]*é€²åº¦[^\n]*',
            r'å·¥ç¨‹[^\n]*é€²åº¦[^\n]*',
            r'æ–œå¡[^\n]*ç¶­ä¿®[^\n]*',
            r'æŸ¥è©¢äºº[^\n]*æ–œå¡[^\n]*',
        ]
        
        for pattern in concern_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                key_info['main_concern'] = match.group(0).strip()
                break
        
        # æå–æè¿°ä¿¡æ¯
        description_patterns = [
            r'Description[ï¼š:]\s*([^\n]+)',
            r'æŸ¥è©¢äººæä¾›æ–œå¡ç·¨è™Ÿç‚º([^\n]+)',
            r'æ–œå¡å±¬([^\n]+)è² è²¬',
        ]
        
        for pattern in description_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                key_info['description'] = match.group(1).strip() if match.groups() else match.group(0).strip()
                break
        
        return key_info
    
    def _extract_keywords(self, content: str) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []
        
        # å®šä¹‰å…³é”®è¯æ¨¡å¼
        keyword_patterns = [
            r'æ–œå¡[^\s]*',
            r'ç¶­ä¿®[^\s]*',
            r'å·¥ç¨‹[^\s]*',
            r'é€²åº¦[^\s]*',
            r'æŸ¥è©¢[^\s]*',
            r'11SW[^\s]*',
            r'ç·¨è™Ÿ[^\s]*',
        ]
        
        for pattern in keyword_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            keywords.extend(matches)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_keywords = list(set(keywords))
        return unique_keywords[:10]
    
    def analyze_email_content(self, email_content: str) -> str:
        """åˆ†æé‚®ä»¶å†…å®¹"""
        if not email_content or not email_content.strip():
            return "æ— é‚®ä»¶å†…å®¹"
        
        # æå–é‚®ä»¶ä¸­çš„å…³é”®ä¿¡æ¯
        email_info = self._extract_email_information(email_content)
        
        if email_info:
            return f"é‚®ä»¶å†…å®¹: {email_info}"
        
        return "é‚®ä»¶å†…å®¹åˆ†æå®Œæˆ"
    
    def _extract_email_information(self, email_content: str) -> Optional[str]:
        """æå–é‚®ä»¶ä¿¡æ¯"""
        # æŸ¥æ‰¾é‚®ä»¶ä¸­çš„å…³é”®ä¿¡æ¯
        patterns = [
            r'æŸ¥è©¢[^\n]*æ–œå¡[^\n]*',
            r'æ–œå¡[^\n]*ç¶­ä¿®[^\n]*',
            r'å·¥ç¨‹[^\n]*é€²åº¦[^\n]*',
            r'11SW[^\n]*',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, email_content, re.IGNORECASE)
            if match:
                return match.group(0).strip()
        
        return None


# å…¨å±€å¤„ç†å™¨å®ä¾‹
_nlp_processor = None

def get_nlp_enhanced_nature_of_request(txt_content: str, email_content: str = None) -> str:
    """
    è·å–NLPå¢å¼ºçš„Nature of Requestï¼ˆå…¨å±€å‡½æ•°ï¼‰
    
    Args:
        txt_content (str): TXTæ–‡ä»¶å†…å®¹
        email_content (str): é‚®ä»¶å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        
    Returns:
        str: æ™ºèƒ½æ€»ç»“çš„è¯‰æ±‚å†…å®¹
    """
    global _nlp_processor
    
    if _nlp_processor is None:
        _nlp_processor = NLPEnhancedProcessor()
    
    return _nlp_processor.extract_nature_of_request(txt_content, email_content)


def analyze_email_content(email_content: str) -> str:
    """
    åˆ†æé‚®ä»¶å†…å®¹ï¼ˆå…¨å±€å‡½æ•°ï¼‰
    
    Args:
        email_content (str): é‚®ä»¶å†…å®¹
        
    Returns:
        str: åˆ†æç»“æœ
    """
    global _nlp_processor
    
    if _nlp_processor is None:
        _nlp_processor = NLPEnhancedProcessor()
    
    return _nlp_processor.analyze_email_content(email_content)


if __name__ == "__main__":
    # æµ‹è¯•NLPå¢å¼ºå¤„ç†å™¨
    print("=" * 70)
    print("NLPå¢å¼ºå¤„ç†å™¨æµ‹è¯•")
    print("=" * 70)
    
    # æµ‹è¯•TXTæ–‡ä»¶
    txt_file = "exampleInput/txt/3-3YXXSJV.txt"
    email_file = "exampleInput/txt/emailcontent_3-3YXXSJV.txt"
    
    if os.path.exists(txt_file):
        print(f"\\næµ‹è¯•TXTæ–‡ä»¶: {txt_file}")
        with open(txt_file, 'r', encoding='utf-8') as f:
            txt_content = f.read()
        
        email_content = None
        if os.path.exists(email_file):
            print(f"æµ‹è¯•é‚®ä»¶æ–‡ä»¶: {email_file}")
            with open(email_file, 'r', encoding='utf-8') as f:
                email_content = f.read()
        
        # ä½¿ç”¨NLPå¢å¼ºå¤„ç†
        nature_of_request = get_nlp_enhanced_nature_of_request(txt_content, email_content)
        print(f"\\nNLPå¢å¼ºæ€»ç»“ç»“æœ:")
        print(f"Nature of Request: {nature_of_request}")
    else:
        print("æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")