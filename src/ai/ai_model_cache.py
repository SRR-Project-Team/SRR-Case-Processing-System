"""
AIæ¨¡å‹ç¼“å­˜ç®¡ç†å™¨
ç”¨äºç¼“å­˜å·²åˆå§‹åŒ–çš„AIæ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½ï¼Œæé«˜å¤„ç†é€Ÿåº¦
"""

import time
from typing import Dict, Any, Optional
import threading

class AIModelCache:
    """AIæ¨¡å‹ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self):
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.Lock()
        self._cache_timeout = 1800  # 30åˆ†é’Ÿç¼“å­˜è¶…æ—¶
    
    def get_model(self, model_key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜çš„æ¨¡å‹"""
        with self._lock:
            if model_key in self._cache:
                cache_entry = self._cache[model_key]
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if time.time() - cache_entry['timestamp'] < self._cache_timeout:
                    print(f"ğŸš€ ä½¿ç”¨ç¼“å­˜çš„{model_key}æ¨¡å‹")
                    return cache_entry['model']
                else:
                    # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤
                    del self._cache[model_key]
                    print(f"â° {model_key}æ¨¡å‹ç¼“å­˜å·²è¿‡æœŸ")
            return None
    
    def set_model(self, model_key: str, model: Any):
        """ç¼“å­˜æ¨¡å‹"""
        with self._lock:
            self._cache[model_key] = {
                'model': model,
                'timestamp': time.time()
            }
            print(f"ğŸ’¾ ç¼“å­˜{model_key}æ¨¡å‹")
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self._cache.clear()
            print("ğŸ—‘ï¸ æ¸…ç©ºAIæ¨¡å‹ç¼“å­˜")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        with self._lock:
            info = {
                'cached_models': list(self._cache.keys()),
                'cache_count': len(self._cache),
                'cache_timeout': self._cache_timeout
            }
            return info

# å…¨å±€ç¼“å­˜å®ä¾‹
_global_cache: Optional[AIModelCache] = None

def get_ai_model_cache() -> AIModelCache:
    """è·å–å…¨å±€AIæ¨¡å‹ç¼“å­˜å®ä¾‹"""
    global _global_cache
    if _global_cache is None:
        _global_cache = AIModelCache()
    return _global_cache

def clear_ai_model_cache():
    """æ¸…ç©ºå…¨å±€AIæ¨¡å‹ç¼“å­˜"""
    global _global_cache
    if _global_cache:
        _global_cache.clear_cache()

# ä¾¿æ·å‡½æ•°
def get_cached_model(model_key: str) -> Optional[Any]:
    """è·å–ç¼“å­˜çš„æ¨¡å‹"""
    cache = get_ai_model_cache()
    return cache.get_model(model_key)

def cache_model(model_key: str, model: Any):
    """ç¼“å­˜æ¨¡å‹"""
    cache = get_ai_model_cache()
    cache.set_model(model_key, model)

def get_cache_info() -> Dict[str, Any]:
    """è·å–ç¼“å­˜ä¿¡æ¯"""
    cache = get_ai_model_cache()
    return cache.get_cache_info()

if __name__ == "__main__":
    # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    print("ğŸ§ª æµ‹è¯•AIæ¨¡å‹ç¼“å­˜...")
    
    # æ¨¡æ‹Ÿæ¨¡å‹
    class MockModel:
        def __init__(self, name):
            self.name = name
            print(f"ğŸ”§ åˆå§‹åŒ–æ¨¡å‹: {name}")
    
    # æµ‹è¯•ç¼“å­˜
    cache = get_ai_model_cache()
    
    # ç¬¬ä¸€æ¬¡è·å–ï¼ˆåº”è¯¥è¿”å›Noneï¼‰
    model1 = cache.get_model('test_model')
    print(f"ç¬¬ä¸€æ¬¡è·å–: {model1}")
    
    # ç¼“å­˜æ¨¡å‹
    mock_model = MockModel('TestModel')
    cache.set_model('test_model', mock_model)
    
    # ç¬¬äºŒæ¬¡è·å–ï¼ˆåº”è¯¥è¿”å›ç¼“å­˜çš„æ¨¡å‹ï¼‰
    model2 = cache.get_model('test_model')
    print(f"ç¬¬äºŒæ¬¡è·å–: {model2.name if model2 else None}")
    
    # è·å–ç¼“å­˜ä¿¡æ¯
    info = cache.get_cache_info()
    print(f"ç¼“å­˜ä¿¡æ¯: {info}")
    
    print("âœ… AIæ¨¡å‹ç¼“å­˜æµ‹è¯•å®Œæˆ")
