"""
SRR Case Processing API Main Program

This program provides RESTful API interfaces for processing SRR case TXT files and extracting structured data.
Adopts modular design, separating data extraction and output logic into independent modules.

Main functions:
1. Receive TXT file uploads
2. Validate file types
3. Call data extraction modules to process file content
4. Call output modules to format results
5. Return JSON format processing results

API endpoints:
- POST /api/process-srr-file: Process SRR case files
- GET /health: Health check

Author: Project3 Team
Version: 1.0
"""
from fastapi import FastAPI, UploadFile, File
from typing import List, Dict, Any
from fastapi.middleware.cors import CORSMiddleware
import os
import tempfile

# Import custom modules
# Set Python path to import project modules
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import core processing modules
from core.extractFromTxt import extract_case_data_from_txt  # TXT file processor
from core.extractFromTMO import extract_case_data_from_pdf as extract_tmo_data  # TMO PDF processor
from core.extractFromRCC import extract_case_data_from_pdf as extract_rcc_data  # RCC PDF processor
from core.output import (  # Output formatting module
    create_structured_data, 
    create_success_result, 
    create_error_result,
    validate_file_type,
    get_file_type_error_message,
    ProcessingResult
)
from utils.smart_file_pairing import SmartFilePairing  # Smart file pairing utility
from utils.file_utils import read_file_with_encoding

# Set database module path
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from database import get_db_manager  # dataåº“ç®¡ç†å™¨

# initializedataåº“manager
# createå…¨å±€dataåº“managerinstanceï¼Œç”¨äºprocessæ¡ˆä»¶dataçš„storageå’Œæ£€ç´¢
db_manager = get_db_manager()

# importLLMservice
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from src.services.llm_service import get_llm_service
from config.settings import LLM_API_KEY

# createFastAPIåº”ç”¨instance
# configurationAPIåŸºæœ¬informationï¼ŒåŒ…æ‹¬æ ‡é¢˜å’Œç‰ˆæœ¬å·
app = FastAPI(
    title="SRRæ¡ˆä»¶processAPIï¼ˆA-Qæ–°è§„åˆ™ï¼‰", 
    version="1.0",
    description="æ™ºèƒ½SRRæ¡ˆä»¶processç³»ç»Ÿï¼Œæ”¯æŒTXTã€TMO PDFã€RCC PDFæ–‡ä»¶æ ¼å¼"
)

# configurationCORSmiddleware
# å…è®¸å‰ç«¯åº”ç”¨ï¼ˆReactï¼‰CORSè®¿é—®API
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],  # å‰ç«¯å¼€å‘serviceå™¨åœ°å€
    allow_credentials=True,  # å…è®¸æºå¸¦è®¤è¯information
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰HTTPmethodï¼ˆGETã€POSTç­‰ï¼‰
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´
)

# åœ¨åº”ç”¨å¯åŠ¨æ—¶initializeLLMservice
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    # initializeLLMservice
    from src.services.llm_service import init_llm_service
    from config.settings import LLM_PROVIDER, OPENAI_PROXY_URL, OPENAI_USE_PROXY
    init_llm_service(LLM_API_KEY, LLM_PROVIDER, OPENAI_PROXY_URL, OPENAI_USE_PROXY)
    
    # Initialize historical case matcher (integrates Excel/CSV historical data)
    from src.services.historical_case_matcher import init_historical_matcher
    import os
    data_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'data')
    db_path = os.path.join(data_dir, 'srr_cases.db')
    init_historical_matcher(data_dir, db_path)
    print("âœ… Historical case matcher initialized with Excel/CSV data")

# createä¸´æ—¶ç›®å½•
# ç”¨äºstorageuploadçš„fileï¼Œprocesså®Œæˆåautomaticcleanup
TEMP_DIR = tempfile.mkdtemp()
print(f"ğŸ“ ä¸´æ—¶æ–‡ä»¶ç›®å½•: {TEMP_DIR}")


def determine_file_processing_type(filename: str, content_type: str) -> str:
    """
    æ ¹æ®æ–‡ä»¶åå’Œå†…å®¹classå‹ç¡®å®šprocessæ–¹å¼
    
    Args:
        filename (str): æ–‡ä»¶å
        content_type (str): æ–‡ä»¶MIMEclasså‹
        
    Returns:
        str: processclasså‹ ("txt", "tmo", "rcc", "unknown")
    """
    # checkfileæ‰©å±•å
    if filename.lower().endswith('.txt'):
        return "txt"
    elif filename.lower().endswith('.pdf'):
        # æ ¹æ®fileåå‰ç¼€åˆ¤æ–­PDFclasså‹
        if filename.upper().startswith('ASD'):
            return "tmo"
        elif filename.upper().startswith('RCC'):
            return "rcc"
        else:
            return "unknown"
    else:
        return "unknown"


def validate_file_type_extended(content_type: str, filename: str) -> bool:
    """
    æ‰©å±•çš„æ–‡ä»¶classå‹validateï¼Œæ”¯æŒTXTå’ŒPDFæ–‡ä»¶
    
    Args:
        content_type (str): æ–‡ä»¶MIMEclasså‹
        filename (str): æ–‡ä»¶å
        
    Returns:
        bool: æ˜¯å¦ä¸ºæ”¯æŒçš„æ–‡ä»¶classå‹
    """
    # æ”¯æŒçš„fileclasså‹
    supported_types = ["text/plain", "application/pdf"]
    return content_type in supported_types


def get_file_type_error_message_extended() -> str:
    """
    è·å–æ‰©å±•çš„æ–‡ä»¶classå‹errorinformation
    
    Returns:
        str: æ–‡ä»¶classå‹errorinformation
    """
    return "ä»…æ”¯æŒTXTå’ŒPDFæ–‡ä»¶æ ¼å¼"


async def process_paired_txt_file(main_file_path: str, email_file_path: str = None) -> dict:
    """
    processé…å¯¹çš„TXTæ–‡ä»¶ï¼ˆåŒ…å«å¯é€‰çš„é‚®ä»¶æ–‡ä»¶ï¼‰
    
    Args:
        main_file_path: ä¸»TXTfile path
        email_file_path: é‚®ä»¶file pathï¼ˆå¯é€‰ï¼‰
        
    Returns:
        dict: extractçš„æ¡ˆä»¶data
    """
    if email_file_path:
        # å¦‚æœæœ‰é‚®ä»¶fileï¼Œéœ€è¦manualprocessé…å¯¹
        from core.extractFromTxt import extract_case_data_with_email
        from utils.file_utils import read_file_with_encoding
        
        # readfileå†…å®¹
        main_content = read_file_with_encoding(main_file_path)
        email_content = read_file_with_encoding(email_file_path)
        
        # ä½¿ç”¨é…å¯¹process
        return extract_case_data_with_email(main_content, email_content, main_content)
    else:
        # å•ç‹¬processTXTfileï¼ˆä¼šautomaticæ£€æµ‹é‚®ä»¶fileï¼‰
        return extract_case_data_from_txt(main_file_path)


# æ·»åŠ summarizefunctionfunction
async def generate_file_summary(file_content: str, filename: str, file_path: str = None) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ–‡ä»¶å†…å®¹summarize
    
    Args:
        file_content: æ–‡ä»¶å†…å®¹
        filename: æ–‡ä»¶å
        file_path: file pathï¼ˆå¯é€‰ï¼Œç”¨äºç›´æ¥æ–‡ä»¶processï¼‰
        
    Returns:
        åŒ…å«summarizeresultçš„å­—å…¸
    """
    try:
        # getLLMservice
        llm = get_llm_service()
        
        # ä¼˜å…ˆä½¿ç”¨file pathè¿›è¡Œsummarizeï¼ˆæ”¯æŒPDFç­‰å¤æ‚fileï¼‰
        if file_path:
            summary = llm.summarize_file(file_path, max_length=150)
        else:
            # ä½¿ç”¨text contentè¿›è¡Œsummarize
            summary = llm.summarize_text(file_content, max_length=150)
        
        if summary:
            return {
                "success": True,
                "summary": summary,
                "filename": filename,
                "source": "AI Summary"
            }
        else:
            return {
                "success": False,
                "error": "summarizeç”Ÿæˆfailed",
                "filename": filename
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": f"summarizeprocessexception: {str(e)}",
            "filename": filename
        }

@app.post("/api/process-srr-file", response_model=ProcessingResult)
async def process_srr_file(file: UploadFile = File(...)):
    """
    processSRRæ¡ˆä»¶æ–‡ä»¶ï¼ŒæŒ‰æ–°A-Qè§„åˆ™ç”Ÿæˆç»“æ„åŒ–data
    
    æ¥æ”¶ä¸Šä¼ çš„TXTæˆ–PDFæ–‡ä»¶ï¼Œæ ¹æ®æ–‡ä»¶classå‹å’Œæ–‡ä»¶åè‡ªåŠ¨é€‰æ‹©ç›¸åº”çš„processmoduleï¼š
    - TXTæ–‡ä»¶ï¼šä½¿ç”¨extractFromTxtmodule
    - ASDå¼€å¤´çš„PDFæ–‡ä»¶ï¼šä½¿ç”¨extractFromTMOmodule
    - RCCå¼€å¤´çš„PDFæ–‡ä»¶ï¼šä½¿ç”¨extractFromRCCmodule
    
    processæµç¨‹ï¼š
    1. validateæ–‡ä»¶classå‹ï¼ˆæ”¯æŒtext/plainå’Œapplication/pdfï¼‰
    2. æ ¹æ®æ–‡ä»¶åç¡®å®šprocessclasså‹
    3. ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
    4. è°ƒç”¨ç›¸åº”çš„extractmodule
    5. è°ƒç”¨outputmoduleåˆ›å»ºç»“æ„åŒ–data
    6. returnprocessresult
    7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    
    Args:
        file (UploadFile): ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆTXTæˆ–PDFï¼‰
        
    Returns:
        ProcessingResult: åŒ…å«processçŠ¶æ€å’Œç»“æ„åŒ–dataçš„å“åº”object
        
    Raises:
        Exception: æ–‡ä»¶processè¿‡ç¨‹ä¸­çš„ä»»ä½•erroréƒ½ä¼šè¢«æ•è·å¹¶returnerrorresult
        
    Example:
        POST /api/process-srr-file
        Content-Type: multipart/form-data
        Body: file=ASD-WC-20250089-PP.pdf
        
        Response:
        {
            "filename": "ASD-WC-20250089-PP.pdf",
            "status": "success",
            "message": "SRRæ¡ˆä»¶processsuccess",
            "structured_data": {
                "A_date_received": "2025-01-21T00:00:00",
                "B_source": "TMO",
                ...
            }
        }
    """
    try:
        # validatefileclasså‹
        if not validate_file_type_extended(file.content_type, file.filename):
            return create_error_result(file.filename, get_file_type_error_message_extended())
        
        # ç¡®å®šprocessclasså‹
        processing_type = determine_file_processing_type(file.filename, file.content_type)
        
        if processing_type == "unknown":
            return create_error_result(
                file.filename, 
                f"ä¸æ”¯æŒçš„æ–‡ä»¶classå‹æˆ–æ–‡ä»¶åæ ¼å¼ã€‚æ”¯æŒï¼šTXTæ–‡ä»¶ï¼Œæˆ–ASD/RCCå¼€å¤´çš„PDFæ–‡ä»¶"
            )
        
        # saveuploadçš„fileåˆ°ä¸´æ—¶ç›®å½•
        file_path = os.path.join(TEMP_DIR, file.filename)
        with open(file_path, "wb") as buffer:
            buffer.write(await file.read())
        
        # æ ¹æ®processclasså‹è°ƒç”¨ç›¸åº”çš„extractmodule
        if processing_type == "txt":
            # processTXTfile (ä½¿ç”¨æ™ºèƒ½encodingæ£€æµ‹)
            extracted_data = extract_case_data_from_txt(file_path)
            
        elif processing_type == "tmo":
            # processTMO PDFfile
            extracted_data = extract_tmo_data(file_path)
            
        elif processing_type == "rcc":
            # processRCC PDFfile
            extracted_data = extract_rcc_data(file_path)
            
        else:
            return create_error_result(file.filename, "æœªçŸ¥çš„processclasså‹")
        
        # ä½¿ç”¨outputmodulecreateç»“æ„åŒ–data
        structured_data = create_structured_data(extracted_data)

        # saveæ¡ˆä»¶dataåˆ°dataåº“
        try:
            case_data = {
                'A_date_received': structured_data.A_date_received,
                'B_source': structured_data.B_source,
                'C_case_number': structured_data.C_case_number,
                'D_type': structured_data.D_type,
                'E_caller_name': structured_data.E_caller_name,
                'F_contact_no': structured_data.F_contact_no,
                'G_slope_no': structured_data.G_slope_no,
                'H_location': structured_data.H_location,
                'I_nature_of_request': structured_data.I_nature_of_request,
                'J_subject_matter': structured_data.J_subject_matter,
                'K_10day_rule_due_date': structured_data.K_10day_rule_due_date,
                'L_icc_interim_due': structured_data.L_icc_interim_due,
                'M_icc_final_due': structured_data.M_icc_final_due,
                'N_works_completion_due': structured_data.N_works_completion_due,
                'O1_fax_to_contractor': structured_data.O1_fax_to_contractor,
                'O2_email_send_time': structured_data.O2_email_send_time,
                'P_fax_pages': structured_data.P_fax_pages,
                'Q_case_details': structured_data.Q_case_details,
                'original_filename': file.filename,
                'file_type': processing_type
            }
            case_id = db_manager.save_case(case_data)
            print(f"âœ… æ¡ˆä»¶ä¿å­˜successï¼ŒID: {case_id}")
        except Exception as db_error:
            print(f"âš ï¸ dataåº“ä¿å­˜failed: {db_error}")

        # read file content for summary
        try:
            file_content = read_file_with_encoding(file_path)
            
            # generate AI summary (ä¼ å…¥file pathä»¥æ”¯æŒPDFç­‰å¤æ‚file)
            summary_result = await generate_file_summary(file_content, file.filename, file_path)
            
        except Exception as e:
            # summary failed independent of main functionality
            summary_result = {
                "success": False,
                "error": f"summarizeç”Ÿæˆfailed: {str(e)}"
            }

        # returnsuccessresult
        return create_success_result(file.filename, structured_data, summary_result)
        
        
    except Exception as e:
        # æ•è·æ‰€æœ‰exceptionå¹¶returnerrorresult
        return create_error_result(
            file.filename if 'file' in locals() else "unknown",
            f"processfailed: {str(e)}"
        )
    finally:
        # cleanupä¸´æ—¶file
        if 'file_path' in locals() and os.path.exists(file_path):
            os.remove(file_path)


@app.post("/api/process-multiple-files")
async def process_multiple_files(files: List[UploadFile] = File(...)):
    """
    æ™ºèƒ½æ‰¹é‡processå¤šä¸ªSRRæ¡ˆä»¶æ–‡ä»¶
    
    æ”¯æŒæ™ºèƒ½æ–‡ä»¶é…å¯¹ï¼šè‡ªåŠ¨è¯†åˆ«TXTæ¡ˆä»¶æ–‡ä»¶å’Œå¯¹åº”çš„é‚®ä»¶æ–‡ä»¶ï¼Œè¿›è¡Œé…å¯¹processã€‚
    - TXTæ–‡ä»¶ + å¯¹åº”çš„emailcontent_*.txtæ–‡ä»¶ â†’ é…å¯¹processï¼ˆåŒ…å«é‚®ä»¶informationï¼‰
    - å•ç‹¬çš„TXTæ–‡ä»¶ â†’ ç‹¬ç«‹processï¼ˆè‡ªåŠ¨æ£€æµ‹é‚®ä»¶æ–‡ä»¶ï¼‰
    - å•ç‹¬çš„PDFæ–‡ä»¶ â†’ ç‹¬ç«‹process
    - ç‹¬ç«‹çš„é‚®ä»¶æ–‡ä»¶ â†’ è·³è¿‡process
    
    Args:
        files: ä¸Šä¼ çš„æ–‡ä»¶åˆ—table
        
    Returns:
        dict: åŒ…å«æ‰€æœ‰æ–‡ä»¶processresultçš„å­—å…¸
        {
            "total_files": ä¸Šä¼ çš„æ–‡ä»¶æ€»æ•°,
            "processed_cases": å®é™…processçš„æ¡ˆä»¶æ•°,
            "successful": successprocessçš„æ¡ˆä»¶æ•°,
            "failed": failedçš„æ¡ˆä»¶æ•°,
            "skipped": è·³è¿‡çš„æ–‡ä»¶æ•°,
            "results": [
                {
                    "case_id": "æ¡ˆä»¶ID",
                    "main_file": "ä¸»æ–‡ä»¶å",
                    "email_file": "é‚®ä»¶æ–‡ä»¶åï¼ˆå¦‚æœæœ‰ï¼‰",
                    "status": "success|error|skipped",
                    "message": "processæ¶ˆæ¯",
                    "structured_data": {...} // ä»…successæ—¶åŒ…å«
                },
                ...
            ]
        }
    """
    if not files:
        return {
            "total_files": 0,
            "processed_cases": 0,
            "successful": 0,
            "failed": 0,
            "skipped": 0,
            "results": [{
                "case_id": "none",
                "main_file": "none",
                "email_file": None,
                "status": "error",
                "message": "æ²¡æœ‰ä¸Šä¼ ä»»ä½•æ–‡ä»¶"
            }]
        }
    
    print(f"ğŸš€ å¼€å§‹æ™ºèƒ½æ‰¹é‡process {len(files)} ä¸ªæ–‡ä»¶...")
    
    # ç¬¬ä¸€æ­¥ï¼šcreateæ™ºèƒ½filepairing
    pairing = SmartFilePairing()
    
    # saveæ‰€æœ‰fileåˆ°ä¸´æ—¶ç›®å½•å¹¶æ·»åŠ åˆ°pairing
    temp_files = {}
    for file in files:
        # validatefileclasså‹
        if not validate_file_type_extended(file.content_type, file.filename):
            print(f"âš ï¸ è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶classå‹: {file.filename}")
            continue
        
        # savefileåˆ°ä¸´æ—¶ç›®å½•
        file_path = os.path.join(TEMP_DIR, file.filename)
        with open(file_path, "wb") as buffer:
            buffer.write(await file.read())
        
        temp_files[file.filename] = file_path
        pairing.add_file(file.filename, file.content_type)
    
    # ç¬¬äºŒæ­¥ï¼šgetæ™ºèƒ½é…å¯¹processè®¡åˆ’
    processing_summary = pairing.get_processing_summary()
    processing_plan = processing_summary['processing_plan']
    
    print(f"ğŸ“‹ æ™ºèƒ½é…å¯¹result:")
    print(f"   - å®Œæ•´é…å¯¹: {processing_summary['txt_with_email']} ä¸ª")
    print(f"   - å•ç‹¬TXT: {processing_summary['txt_only']} ä¸ª")
    print(f"   - è·³è¿‡æ–‡ä»¶: {processing_summary['skipped']} ä¸ª")
    
    # ç¬¬ä¸‰æ­¥ï¼šæŒ‰ç…§processè®¡åˆ’æ‰§è¡Œ
    results = []
    successful_count = 0
    failed_count = 0
    skipped_count = 0
    
    try:
        for i, plan in enumerate(processing_plan, 1):
            case_id = plan['case_id']
            plan_type = plan['type']
            main_file = plan['main_file']
            email_file = plan.get('email_file')
            
            print(f"\nğŸ“ processè®¡åˆ’ {i}/{len(processing_plan)}: {plan['description']}")
            
            if plan_type == 'skip':
                # è·³è¿‡ç‹¬ç«‹çš„é‚®ä»¶file
                result = {
                    "case_id": case_id,
                    "main_file": main_file.filename,
                    "email_file": None,
                    "status": "skipped",
                    "message": f"è·³è¿‡ç‹¬ç«‹é‚®ä»¶æ–‡ä»¶ï¼ˆæ— å¯¹åº”TXTæ–‡ä»¶ï¼‰"
                }
                results.append(result)
                skipped_count += 1
                print(f"â­ï¸ è·³è¿‡æ–‡ä»¶: {main_file.filename}")
                continue
            
            try:
                # getfile path
                main_file_path = temp_files.get(main_file.filename)
                email_file_path = temp_files.get(email_file.filename) if email_file else None
                
                if not main_file_path or not os.path.exists(main_file_path):
                    raise FileNotFoundError(f"ä¸»æ–‡ä»¶ä¸å­˜åœ¨: {main_file.filename}")
                
                # æ ¹æ®fileclasså‹process
                if main_file.filename.lower().endswith('.txt'):
                    # processTXTfileï¼ˆå¯èƒ½åŒ…å«é‚®ä»¶é…å¯¹ï¼‰
                    extracted_data = await process_paired_txt_file(main_file_path, email_file_path)
                    
                elif main_file.filename.lower().endswith('.pdf'):
                    # processPDFfile
                    processing_type = determine_file_processing_type(main_file.filename, main_file.content_type)
                    
                    if processing_type == "tmo":
                        extracted_data = extract_tmo_data(main_file_path)
                    elif processing_type == "rcc":
                        extracted_data = extract_rcc_data(main_file_path)
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„PDFæ–‡ä»¶classå‹: {main_file.filename}")
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {main_file.filename}")
                
                # createç»“æ„åŒ–data
                structured_data = create_structured_data(extracted_data)
                
                # successresult
                result = {
                    "case_id": case_id,
                    "main_file": main_file.filename,
                    "email_file": email_file.filename if email_file else None,
                    "status": "success",
                    "message": f"æ¡ˆä»¶ {case_id} processsuccess" + (f"ï¼ˆåŒ…å«é‚®ä»¶informationï¼‰" if email_file else ""),
                    "structured_data": structured_data
                }
                results.append(result)
                successful_count += 1
                print(f"âœ… æ¡ˆä»¶ {case_id} processsuccess")
        
            except Exception as e:
                # processfailed
                result = {
                    "case_id": case_id,
                    "main_file": main_file.filename,
                    "email_file": email_file.filename if email_file else None,
                    "status": "error",
                    "message": f"processfailed: {str(e)}"
                }
                results.append(result)
                failed_count += 1
                print(f"âŒ æ¡ˆä»¶ {case_id} processfailed: {str(e)}")
    
    except Exception as outer_e:
        print(f"âŒ æ‰¹é‡processè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡error: {str(outer_e)}")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šçš„errorprocessé€»è¾‘
    
    finally:
        # cleanupæ‰€æœ‰ä¸´æ—¶file
        for file_path in temp_files.values():
            if os.path.exists(file_path):
                os.remove(file_path)
    
    processed_cases = successful_count + failed_count
    print(f"\nğŸ“Š æ™ºèƒ½æ‰¹é‡processå®Œæˆ:")
    print(f"   - ä¸Šä¼ æ–‡ä»¶: {len(files)} ä¸ª")
    print(f"   - processæ¡ˆä»¶: {processed_cases} ä¸ª")
    print(f"   - success: {successful_count} ä¸ª")
    print(f"   - failed: {failed_count} ä¸ª")
    print(f"   - è·³è¿‡: {skipped_count} ä¸ª")
    
    return {
        "total_files": len(files),
        "processed_cases": processed_cases,
        "successful": successful_count,
        "failed": failed_count,
        "skipped": skipped_count,
        "results": results
    }


# æ¡ˆä»¶ç®¡ç†
@app.get("/api/cases")
async def get_cases(limit: int = 100, offset: int = 0):
    """è·å–æ¡ˆä»¶åˆ—table"""
    cases = db_manager.get_cases(limit, offset)
    return {"cases": cases, "total": len(cases)}

@app.get("/api/cases/{case_id}")
async def get_case(case_id: int):
    """è·å–å•ä¸ªæ¡ˆä»¶"""
    case = db_manager.get_case(case_id)
    if case:
        return case
    return {"error": "æ¡ˆä»¶ä¸å­˜åœ¨"}

@app.get("/api/cases/search")
async def search_cases(q: str):
    """æœç´¢æ¡ˆä»¶"""
    cases = db_manager.search_cases(q)
    return {"cases": cases, "query": q}

@app.post("/api/find-similar-cases")
async def find_similar_cases(case_data: dict):
    """
    Find similar historical cases based on current case information
    Searches ONLY historical Excel/CSV data (database excluded):
    - Slopes Complaints 2021 (4,047 cases)
    - SRR data 2021-2024 (1,251 cases)
    Total: 5,298 historical cases
    
    Args:
        case_data: Dictionary containing case information to match against
        
    Returns:
        dict: Similar cases with similarity scores and match details
    """
    try:
        from src.services.historical_case_matcher import get_historical_matcher
        
        matcher = get_historical_matcher()
        
        # Get parameters
        limit = case_data.get('limit', 10)
        min_similarity = case_data.get('min_similarity', 0.3)
        
        # Find similar cases across all historical data
        similar_cases = matcher.find_similar_cases(
            current_case=case_data,
            limit=limit,
            min_similarity=min_similarity
        )
        
        return {
            "status": "success",
            "total_found": len(similar_cases),
            "similar_cases": similar_cases,
            "search_criteria": {
                "location": case_data.get('H_location'),
                "slope_no": case_data.get('G_slope_no'),
                "caller_name": case_data.get('E_caller_name'),
                "subject_matter": case_data.get('J_subject_matter')
            },
            "data_sources": {
                "slopes_complaints_2021": "4,047 cases",
                "srr_data_2021_2024": "1,251 cases",
                "total_searchable": "5,298 historical cases (database excluded)"
            }
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "message": f"Failed to find similar cases: {str(e)}"
        }


@app.get("/api/case-statistics")
async def get_case_statistics(
    location: str = None,
    slope_no: str = None,
    venue: str = None
):
    """
    Get comprehensive statistics from historical Excel/CSV data ONLY
    Searches across (database excluded):
    - Slopes Complaints 2021 (4,047 cases)
    - SRR data 2021-2024 (1,251 cases)
    Total: 5,298 historical cases
    
    Query parameters:
        location: Location to filter by
        slope_no: Slope number to filter by
        venue: Venue name to filter by
        
    Returns:
        dict: Comprehensive statistics from historical data only
    """
    try:
        from src.services.historical_case_matcher import get_historical_matcher
        
        matcher = get_historical_matcher()
        
        stats = matcher.get_case_statistics(
            location=location,
            slope_no=slope_no,
            venue=venue
        )
        
        return {
            "status": "success",
            "statistics": stats
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "message": f"Failed to get statistics: {str(e)}"
        }


@app.get("/api/tree-info")
async def get_tree_info(slope_no: str):
    """
    Get tree information for a specific slope
    Searches tree inventory (32405 trees)
    
    Query parameters:
        slope_no: Slope number to search for
        
    Returns:
        dict: List of trees on the slope with details
    """
    try:
        from src.services.historical_case_matcher import get_historical_matcher
        
        matcher = get_historical_matcher()
        trees = matcher.get_tree_info(slope_no)
        
        return {
            "status": "success",
            "slope_no": slope_no,
            "total_trees": len(trees),
            "trees": trees
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"Failed to get tree information: {str(e)}"
        }


@app.get("/api/location-slopes")
async def get_location_slopes(location: str):
    """
    Get slope numbers associated with a location
    Uses historical learning from 5,298 cases
    
    Query parameters:
        location: Location name or partial match
        
    Returns:
        dict: List of slope numbers found at this location
    """
    try:
        from src.services.historical_case_matcher import get_historical_matcher
        
        matcher = get_historical_matcher()
        slopes = matcher.get_slopes_for_location(location)
        
        return {
            "status": "success",
            "location": location,
            "total_slopes": len(slopes),
            "slopes": slopes,
            "note": "Learned from historical complaint records"
        }
        
    except Exception as e:
        return {
            "status": "error",
            "message": f"Failed to get slopes for location: {str(e)}"
        }


@app.get("/health")
def health_check():
    """
    å¥åº·checkç«¯ç‚¹
    
    ç”¨äºcheckAPIserviceæ˜¯å¦æ­£å¸¸è¿è¡Œï¼Œå¯ç”¨äºè´Ÿè½½å‡è¡¡å™¨æˆ–ç›‘æ§ç³»ç»Ÿ
    æ”¯æŒTXTå’ŒPDFæ–‡ä»¶process
    
    Returns:
        dict: åŒ…å«serviceçŠ¶æ€çš„å“åº”
        
    Example:
        GET /health
        
        Response:
        {
            "status": "healthy",
            "message": "SRRæ¡ˆä»¶processAPIè¿è¡Œæ­£å¸¸"
        }
    """
    return {"status": "healthy", "message": "SRRæ¡ˆä»¶processAPIè¿è¡Œæ­£å¸¸ï¼Œæ”¯æŒTXTå’ŒPDFæ–‡ä»¶"}


if __name__ == "__main__":
    """
    ç¨‹åºå…¥å£ç‚¹
    
    å½“ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶å¯åŠ¨FastAPIserviceå™¨
    configurationï¼š
    - ä¸»æœº: 0.0.0.0 (å…è®¸å¤–éƒ¨è®¿é—®)
    - ç«¯å£: 8001
    - è‡ªåŠ¨é‡è½½: å¯ç”¨ (å¼€å‘æ¨¡å¼)
    """
    import uvicorn
    uvicorn.run(app="main:app", host="0.0.0.0", port=8001, reload=True)
    