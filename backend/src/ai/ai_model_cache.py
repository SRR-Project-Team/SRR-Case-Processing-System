"""
AImodelcachemanager
ç”¨äºcacheå·²initializeçš„AImodelï¼Œé¿å…duplicateloadï¼Œæé«˜processspeed
"""

import time
from typing import Dict, Any, Optional
import threading

class AIModelCache:
    """AImodelcacheç®¡ç†å™¨"""
    
    def __init__(self):
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._lock = threading.Lock()
        self._cache_timeout = 1800  # 30åˆ†é’Ÿcacheè¶…æ—¶
    
    def get_model(self, model_key: str) -> Optional[Any]:
        """è·å–cacheçš„model"""
        with self._lock:
            if model_key in self._cache:
                cache_entry = self._cache[model_key]
                # checkcacheæ˜¯å¦è¿‡æœŸ
                if time.time() - cache_entry['timestamp'] < self._cache_timeout:
                    print(f"ğŸš€ ä½¿ç”¨cacheçš„{model_key}model")
                    return cache_entry['model']
                else:
                    # cacheè¿‡æœŸï¼Œdelete
                    del self._cache[model_key]
                    print(f"â° {model_key}modelcacheå·²è¿‡æœŸ")
            return None
    
    def set_model(self, model_key: str, model: Any):
        """cachemodel"""
        with self._lock:
            self._cache[model_key] = {
                'model': model,
                'timestamp': time.time()
            }
            print(f"ğŸ’¾ cache{model_key}model")
    
    def clear_cache(self):
        """æ¸…ç©ºcache"""
        with self._lock:
            self._cache.clear()
            print("ğŸ—‘ï¸ æ¸…ç©ºAImodelcache")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """è·å–cacheinformation"""
        with self._lock:
            info = {
                'cached_models': list(self._cache.keys()),
                'cache_count': len(self._cache),
                'cache_timeout': self._cache_timeout
            }
            return info

# å…¨å±€cacheinstance
_global_cache: Optional[AIModelCache] = None

def get_ai_model_cache() -> AIModelCache:
    """è·å–å…¨å±€AImodelcacheinstance"""
    global _global_cache
    if _global_cache is None:
        _global_cache = AIModelCache()
    return _global_cache

def clear_ai_model_cache():
    """æ¸…ç©ºå…¨å±€AImodelcache"""
    global _global_cache
    if _global_cache:
        _global_cache.clear_cache()

# ä¾¿æ·function
def get_cached_model(model_key: str) -> Optional[Any]:
    """è·å–cacheçš„model"""
    cache = get_ai_model_cache()
    return cache.get_model(model_key)

def cache_model(model_key: str, model: Any):
    """cachemodel"""
    cache = get_ai_model_cache()
    cache.set_model(model_key, model)

def get_cache_info() -> Dict[str, Any]:
    """è·å–cacheinformation"""
    cache = get_ai_model_cache()
    return cache.get_cache_info()

if __name__ == "__main__":
    # testcachefunction
    print("ğŸ§ª æµ‹è¯•AImodelcache...")
    
    # æ¨¡æ‹Ÿmodel
    class MockModel:
        def __init__(self, name):
            self.name = name
            print(f"ğŸ”§ initializemodel: {name}")
    
    # testcache
    cache = get_ai_model_cache()
    
    # ç¬¬ä¸€æ¬¡getï¼ˆåº”è¯¥returnNoneï¼‰
    model1 = cache.get_model('test_model')
    print(f"ç¬¬ä¸€æ¬¡è·å–: {model1}")
    
    # cachemodel
    mock_model = MockModel('TestModel')
    cache.set_model('test_model', mock_model)
    
    # ç¬¬äºŒæ¬¡getï¼ˆåº”è¯¥returncacheçš„modelï¼‰
    model2 = cache.get_model('test_model')
    print(f"ç¬¬äºŒæ¬¡è·å–: {model2.name if model2 else None}")
    
    # getcacheinformation
    info = cache.get_cache_info()
    print(f"cacheinformation: {info}")
    
    print("âœ… AImodelcacheæµ‹è¯•å®Œæˆ")
